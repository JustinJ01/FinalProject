{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ff78d1",
   "metadata": {},
   "source": [
    "# Final Project: Sporting Game Outcome Prediction\n",
    "## Data Loading and Processing\n",
    "\n",
    "This initial section focuses on setting up the environment and defining functions to load and transform the raw play-by-play data into a usable format for game outcome prediction. We will:\n",
    "\n",
    "* **Import Libraries:** Bring in essential libraries like pandas for data manipulation, numpy for numerical operations, matplotlib/seaborn for visualization, and various modules from scikit-learn for modeling and evaluation.\n",
    "* **Define `load_and_process_data` Function:** Create a function to handle a single CSV file. This function will:\n",
    "    * Read the play-by-play data.\n",
    "    * Identify unique teams and attempt robust home/away team assignment (checking for `IsHome` or similar columns, falling back to approximation if needed).\n",
    "    * Iterate through each game, accumulating team statistics (yards, attempts, turnovers, penalties, scores, etc.) by processing individual plays.\n",
    "    * Calculate game-level differential features (e.g., `yards_diff`, `turnovers_diff`, `ypa_diff`) which often provide strong predictive signals.\n",
    "    * Determine the game winner based on simplified score calculation (to be used as the target variable).\n",
    "    * Return a pandas DataFrame where each row represents a single game with its aggregated stats and differentials.\n",
    "* **Define `combine_data_multiple_years` Function:** Create a helper function to apply `load_and_process_data` to multiple yearly CSV files and concatenate the results into a single DataFrame.\n",
    "* **Handle Warnings:** Suppress convergence and user warnings for cleaner output during potentially long computations like grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler # Include both scalers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance # For non-linear feature importance\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Ignore convergence warnings for cleaner output during grid search\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Ignore potential user warnings from libraries\n",
    "\n",
    "# Function to load and process a single year of data\n",
    "def load_and_process_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and process a single year of play-by-play data.\n",
    "    Aggregates statistics to the game level and calculates differentials.\n",
    "    Returns a DataFrame with game-level aggregated statistics.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False) # low_memory=False can help with mixed types\n",
    "        print(f\" Data shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if file not found\n",
    "\n",
    "    # Check for necessary columns early\n",
    "    required_cols = ['GameId', 'OffenseTeam', 'DefenseTeam', 'Yards', 'IsRush', 'IsPass', \n",
    "                     'IsSack', 'IsInterception', 'IsFumble', 'IsPenalty', 'PenaltyTeam', \n",
    "                     'PenaltyYards', 'IsTouchdown', 'PlayType', 'Description', 'Down', 'SeriesFirstDown']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: Missing required columns in {filepath}\")\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        print(f\"Missing columns: {missing}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # --- Home/Away Team Identification --- \n",
    "    # Attempt to identify home team more robustly if possible\n",
    "    # Check common column names indicating home status\n",
    "    home_col = None\n",
    "    if 'IsHome' in df.columns:\n",
    "        home_col = 'IsHome'\n",
    "    elif 'HomeTeam' in df.columns: # If there's a dedicated HomeTeam column per game\n",
    "        home_col = 'HomeTeam_Identifier' # Placeholder, logic needs adjustment based on actual structure\n",
    "        # Example: df['HomeTeam_Identifier'] = (df['OffenseTeam'] == df['HomeTeam']) # Needs actual HomeTeam column\n",
    "    elif 'GameLocation' in df.columns: # If location indicates home/away\n",
    "        home_col = 'IsHome_Location' # Placeholder\n",
    "        # Example: df['IsHome_Location'] = (df['GameLocation'] == 'Home') # Needs actual GameLocation column\n",
    "    \n",
    "    # Group by game to create game-level features\n",
    "    game_stats = {}\n",
    "\n",
    "    # Iterate through each unique game\n",
    "    for game_id in df['GameId'].unique():\n",
    "        game_df = df[df['GameId'] == game_id].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "        # Get unique teams involved in the game\n",
    "        teams = set(game_df['OffenseTeam'].dropna().unique()) | set(game_df['DefenseTeam'].dropna().unique())\n",
    "        teams = [team for team in teams if isinstance(team, str) and team.strip()]\n",
    "\n",
    "        # Skip if we don't have exactly 2 teams identified\n",
    "        if len(teams) != 2:\n",
    "            # print(f\"Skipping GameId {game_id}: Found {len(teams)} teams ({teams})\")\n",
    "            continue\n",
    "            \n",
    "        team1, team2 = teams[0], teams[1]\n",
    "        home_team, away_team = None, None\n",
    "\n",
    "        # Determine Home/Away based on identified column or approximation\n",
    "        if home_col == 'IsHome':\n",
    "            # Find the first play where IsHome is defined for team1 or team2\n",
    "            home_play = game_df[((game_df['OffenseTeam'] == team1) | (game_df['DefenseTeam'] == team1)) & game_df['IsHome'].notna()].iloc[0] if not game_df[((game_df['OffenseTeam'] == team1) | (game_df['DefenseTeam'] == team1)) & game_df['IsHome'].notna()].empty else None\n",
    "            if home_play is not None:\n",
    "                if home_play['IsHome'] == 1:\n",
    "                    home_team, away_team = team1, team2\n",
    "                else:\n",
    "                    home_team, away_team = team2, team1\n",
    "        # Add logic here if using 'HomeTeam_Identifier' or 'IsHome_Location' based on actual data structure\n",
    "        \n",
    "        # If robust identification failed, use approximation\n",
    "        if home_team is None or away_team is None:\n",
    "            # print(f\"Warning: Using approximation for home/away in GameId {game_id}\")\n",
    "            home_team, away_team = team1, team2 # Approximation\n",
    "\n",
    "        # Initialize team stats dictionaries (add new stats)\n",
    "        home_stats = {\n",
    "            'total_yards': 0, 'pass_yards': 0, 'rush_yards': 0,\n",
    "            'pass_attempts': 0, 'rush_attempts': 0, 'turnovers': 0,\n",
    "            'sacks_taken': 0, 'penalties': 0, 'penalty_yards': 0,\n",
    "            'touchdowns': 0, 'field_goals': 0,\n",
    "            'third_down_attempts': 0, 'third_down_conversions': 0, # New\n",
    "            'fg_attempts': 0, 'punt_yards': 0 # New\n",
    "        }\n",
    "        away_stats = home_stats.copy()\n",
    "\n",
    "        # Calculate stats for each team by iterating through plays\n",
    "        for _, play in game_df.iterrows():\n",
    "            off_team = play['OffenseTeam']\n",
    "            if not isinstance(off_team, str) or not off_team.strip():\n",
    "                continue # Skip plays with missing offense team\n",
    "\n",
    "            # Determine which team's stats to update\n",
    "            is_home_offense = (off_team == home_team)\n",
    "            current_team_stats = home_stats if is_home_offense else away_stats\n",
    "            # opp_team_stats = away_stats if is_home_offense else home_stats # Needed if calculating defensive stats explicitly\n",
    "\n",
    "            # --- Accumulate Stats --- \n",
    "            # Yards (handle potential NaN)\n",
    "            yards = play.get('Yards', 0)\n",
    "            if pd.notna(yards):\n",
    "                current_team_stats['total_yards'] += yards\n",
    "                if play.get('IsRush', 0) == 1:\n",
    "                    current_team_stats['rush_yards'] += yards\n",
    "                    current_team_stats['rush_attempts'] += 1\n",
    "                elif play.get('IsPass', 0) == 1:\n",
    "                    current_team_stats['pass_yards'] += yards\n",
    "                    current_team_stats['pass_attempts'] += 1\n",
    "            \n",
    "            # Sacks (recorded against the offense)\n",
    "            if play.get('IsSack', 0) == 1:\n",
    "                current_team_stats['sacks_taken'] += 1\n",
    "                # Ensure pass attempt is counted even on sacks if not already marked as IsPass=1\n",
    "                if play.get('IsPass', 0) != 1:\n",
    "                     current_team_stats['pass_attempts'] += 1\n",
    "            \n",
    "            # Turnovers\n",
    "            if play.get('IsInterception', 0) == 1 or play.get('IsFumble', 0) == 1:\n",
    "                current_team_stats['turnovers'] += 1\n",
    "                \n",
    "            # Penalties (only count if against the current offense team)\n",
    "            if play.get('IsPenalty', 0) == 1 and pd.notna(play.get('PenaltyTeam')) and play['PenaltyTeam'] == off_team:\n",
    "                current_team_stats['penalties'] += 1\n",
    "                # Assuming PenaltyYards are only counted if penalty is accepted (often implied by IsPenalty=1)\n",
    "                penalty_yards = play.get('PenaltyYards', 0)\n",
    "                if pd.notna(penalty_yards):\n",
    "                     current_team_stats['penalty_yards'] += penalty_yards\n",
    "            \n",
    "            # Scoring Plays (ensure it's the offense scoring)\n",
    "            if play.get('IsTouchdown', 0) == 1 and off_team == play.get('OffenseTeam'): \n",
    "                current_team_stats['touchdowns'] += 1\n",
    "            if play.get('PlayType') == 'FIELD GOAL':\n",
    "                current_team_stats['fg_attempts'] += 1 # Count attempt\n",
    "                # Check description for success (might need refinement based on data variations)\n",
    "                if 'IS GOOD' in str(play.get('Description', '')).upper():\n",
    "                    current_team_stats['field_goals'] += 1 # Count make\n",
    "            \n",
    "            # Third Downs\n",
    "            if play.get('Down') == 3:\n",
    "                current_team_stats['third_down_attempts'] += 1\n",
    "                if play.get('SeriesFirstDown', 0) == 1:\n",
    "                    current_team_stats['third_down_conversions'] += 1\n",
    "            \n",
    "            # Punt Yards (assuming 'Yards' on PUNT is net yardage - needs validation)\n",
    "            if play.get('PlayType') == 'PUNT':\n",
    "                 punt_yards = play.get('Yards', 0)\n",
    "                 if pd.notna(punt_yards):\n",
    "                    current_team_stats['punt_yards'] += punt_yards\n",
    "        \n",
    "        # --- Calculate Game Outcome & Differentials ---\n",
    "        # Estimate final score (simplified - ignores PATs, 2pt conversions, safeties)\n",
    "        home_score = home_stats['touchdowns'] * 7 + home_stats['field_goals'] * 3 \n",
    "        away_score = away_stats['touchdowns'] * 7 + away_stats['field_goals'] * 3 \n",
    "\n",
    "        # Determine winner (target variable: 1 if home team wins, 0 otherwise)\n",
    "        # Handle ties: Assign tie as loss for home team (0) or could be excluded/handled differently\n",
    "        winner = 1 if home_score > away_score else 0 \n",
    "        \n",
    "        # Small constant to prevent division by zero\n",
    "        epsilon = 1e-6 \n",
    "\n",
    "        # Calculate efficiency differentials\n",
    "        home_ypa = home_stats['pass_yards'] / (home_stats['pass_attempts'] + epsilon)\n",
    "        away_ypa = away_stats['pass_yards'] / (away_stats['pass_attempts'] + epsilon)\n",
    "        ypa_diff = home_ypa - away_ypa\n",
    "\n",
    "        home_yra = home_stats['rush_yards'] / (home_stats['rush_attempts'] + epsilon)\n",
    "        away_yra = away_stats['rush_yards'] / (away_stats['rush_attempts'] + epsilon)\n",
    "        yra_diff = home_yra - away_yra\n",
    "\n",
    "        home_3rd_conv_pct = home_stats['third_down_conversions'] / (home_stats['third_down_attempts'] + epsilon)\n",
    "        away_3rd_conv_pct = away_stats['third_down_conversions'] / (away_stats['third_down_attempts'] + epsilon)\n",
    "        third_down_conv_pct_diff = home_3rd_conv_pct - away_3rd_conv_pct\n",
    "        \n",
    "        # Calculate derived differentials\n",
    "        total_attempts_diff = (home_stats['pass_attempts'] + home_stats['rush_attempts']) - \\\n",
    "                              (away_stats['pass_attempts'] + away_stats['rush_attempts'])\n",
    "        yards_per_play_diff = (home_stats['total_yards'] - away_stats['total_yards']) / (total_attempts_diff + epsilon)\n",
    "        \n",
    "        # Note: Adding 1 to denominator avoids division by zero if pass_attempts_diff is 0\n",
    "        rush_pass_ratio_diff = (home_stats['rush_attempts'] / (home_stats['pass_attempts'] + epsilon)) - \\\n",
    "                               (away_stats['rush_attempts'] / (away_stats['pass_attempts'] + epsilon))\n",
    "\n",
    "        # Create dictionary of differential features for the game\n",
    "        # Positive difference generally means home team advantage (check turnovers, sacks, penalties)\n",
    "        feature_dict = {\n",
    "            'yards_diff': home_stats['total_yards'] - away_stats['total_yards'],\n",
    "            'pass_yards_diff': home_stats['pass_yards'] - away_stats['pass_yards'],\n",
    "            'rush_yards_diff': home_stats['rush_yards'] - away_stats['rush_yards'],\n",
    "            'turnovers_diff': away_stats['turnovers'] - home_stats['turnovers'], # Fewer TO is better\n",
    "            'sacks_diff': away_stats['sacks_taken'] - home_stats['sacks_taken'], # Fewer sacks taken is better\n",
    "            'penalties_diff': away_stats['penalties'] - home_stats['penalties'], # Fewer penalties is better\n",
    "            'penalty_yards_diff': away_stats['penalty_yards'] - home_stats['penalty_yards'], # Fewer penalty yards is better\n",
    "            'pass_attempts_diff': home_stats['pass_attempts'] - away_stats['pass_attempts'],\n",
    "            'rush_attempts_diff': home_stats['rush_attempts'] - away_stats['rush_attempts'],\n",
    "            # Derived & Efficiency Features\n",
    "            'yards_per_play_diff': yards_per_play_diff,\n",
    "            'rush_pass_ratio_diff': rush_pass_ratio_diff,\n",
    "            'ypa_diff': ypa_diff, # Yards per pass attempt diff\n",
    "            'yra_diff': yra_diff, # Yards per rush attempt diff\n",
    "            'third_down_conv_pct_diff': third_down_conv_pct_diff,\n",
    "            # Other/Special Teams Features\n",
    "            'fg_attempts_diff': home_stats['fg_attempts'] - away_stats['fg_attempts'],\n",
    "            'punt_yards_diff': home_stats['punt_yards'] - away_stats['punt_yards'], # Lower net punt yards might be better (field position)\n",
    "            # --- Leaky Features (for reference, will be dropped before training) ---\n",
    "            'touchdowns_diff': home_stats['touchdowns'] - away_stats['touchdowns'], \n",
    "            'field_goals_diff': home_stats['field_goals'] - away_stats['field_goals'], \n",
    "            'home_score': home_score,\n",
    "            'away_score': away_score,\n",
    "            # --- Target Variable ---\n",
    "            'winner': winner \n",
    "        }\n",
    "\n",
    "        game_stats[game_id] = feature_dict\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    games_df = pd.DataFrame.from_dict(game_stats, orient='index')\n",
    "    games_df.reset_index(inplace=True)\n",
    "    games_df.rename(columns={'index': 'GameId'}, inplace=True)\n",
    "\n",
    "    print(f\" Extracted {len(games_df)} games from {filepath}.\")\n",
    "    return games_df\n",
    "\n",
    "# Function to combine multiple years of data\n",
    "def combine_data_multiple_years(filepaths):\n",
    "    \"\"\"\n",
    "    Load and combine processed game data from multiple CSV files.\n",
    "    \"\"\"\n",
    "    all_games_data = []\n",
    "    for filepath in filepaths:\n",
    "        year_data = load_and_process_data(filepath)\n",
    "        if not year_data.empty:\n",
    "            all_games_data.append(year_data)\n",
    "        else:\n",
    "            print(f\"Skipping empty DataFrame from {filepath}\")\n",
    "\n",
    "    # Combine all years if data was loaded\n",
    "    if not all_games_data:\n",
    "        print(\"No data loaded, returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    combined_data = pd.concat(all_games_data, ignore_index=True)\n",
    "    print(f\"\\nCombined data shape: {combined_data.shape}\")\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba5a85",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection\n",
    "\n",
    "This cell executes the data loading and initial feature preparation steps. It involves:\n",
    "\n",
    "* **Specifying Data Files:** Defining the list of CSV files containing the play-by-play data (currently set to use only `pbp-2024.csv`).\n",
    "* **Loading Data:** Calling the previously defined functions (`load_and_process_data` or `combine_data_multiple_years`) to get the game-level DataFrame `games_df`.\n",
    "* **Handling Missing/Infinite Values:** Replacing any `inf` values generated during calculations with `NaN` and then filling all `NaN` values with 0. This ensures the data is clean for modeling. (Note: Filling with 0 is a simple strategy; more sophisticated imputation could be considered if needed).\n",
    "* **Defining Feature Sets:** Identifying columns that are identifiers (`GameId`), potentially leaky (`touchdowns_diff`, scores, etc.), or the target variable (`winner`).\n",
    "* **Separating X and y:** Creating the feature matrix `X` by selecting all columns except the ones identified to be dropped, and creating the target vector `y` containing the `winner` column.\n",
    "* **Verification:** Printing the final list of features, the number of features, the class balance of the target variable, and checking for any remaining `inf` or `NaN` values in `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths (Adjust as needed - using only 2024 for this example)\n",
    "data_filepaths = [\n",
    "    'pbp-2024.csv' \n",
    "    'pbp-2020.csv',\n",
    "    'pbp-2021.csv',\n",
    "    'pbp-2022.csv',\n",
    "    'pbp-2023.csv'\n",
    "]\n",
    "\n",
    "# Load and combine data\n",
    "# If using only one file, can call load_and_process_data directly\n",
    "if len(data_filepaths) == 1:\n",
    "    games_df = load_and_process_data(data_filepaths[0])\n",
    "else:\n",
    "    games_df = combine_data_multiple_years(data_filepaths)\n",
    "\n",
    "# --- Handle Potential Infinities and NaNs --- \n",
    "# Replace infinities that might arise from division by zero (or near zero) during feature engineering\n",
    "games_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill remaining NaNs. Using 0 might be acceptable for differentials, \n",
    "# but consider median/mean imputation if appropriate for specific features.\n",
    "# Check which columns have NaNs before filling\n",
    "if games_df.isnull().any().any():\n",
    "    print(\"\\nColumns with NaNs before filling:\")\n",
    "    print(games_df.isnull().sum()[games_df.isnull().sum() > 0])\n",
    "    games_df.fillna(0, inplace=True) # Simple fill with 0 for now\n",
    "else:\n",
    "    print(\"\\nNo NaNs found in the aggregated DataFrame.\")\n",
    "\n",
    "# --- Separate Features (X) and Target (y) ---\n",
    "# Define leaky features (directly related to the outcome or score)\n",
    "leaky_features = ['touchdowns_diff', 'field_goals_diff', 'home_score', 'away_score']\n",
    "# Define identifier columns to drop\n",
    "identifier_cols = ['GameId']\n",
    "# Columns to drop before training\n",
    "cols_to_drop = identifier_cols + leaky_features + ['winner'] # Also drop target 'winner'\n",
    "\n",
    "# Define the final feature columns by taking all columns EXCEPT those to drop\n",
    "# Ensure 'winner' column exists before proceeding\n",
    "if 'winner' not in games_df.columns:\n",
    "    print(\"Error: 'winner' column not found in DataFrame. Cannot proceed.\")\n",
    "    # Handle error appropriately, maybe raise exception or exit\n",
    "    X, y = pd.DataFrame(), pd.Series() # Assign empty structures\n",
    "else:\n",
    "    final_feature_cols = [col for col in games_df.columns if col not in cols_to_drop]\n",
    "    X = games_df[final_feature_cols].copy()\n",
    "    y = games_df['winner'].copy()\n",
    "\n",
    "    print(\"\\nFeatures used for prediction:\")\n",
    "    print(X.columns.tolist())\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "    # Check class balance of the target variable\n",
    "    print(\"\\nClass balance (winner):\")\n",
    "    class_balance = y.value_counts(normalize=True)\n",
    "    print(class_balance)\n",
    "\n",
    "    # Final check for infinities/NaNs in features X before splitting\n",
    "    print(f\"\\nInfinities remaining in X before split: {np.any(np.isinf(X.values))}\")\n",
    "    print(f\"NaNs remaining in X before split: {X.isnull().values.any()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59aadbf",
   "metadata": {},
   "source": [
    "## Train-Test Split and Cross-Validation Setup\n",
    "\n",
    "This cell prepares the data for model training and evaluation by splitting it and defining a cross-validation strategy. The key steps are:\n",
    "\n",
    "* **Check Data Validity:** Ensure the feature matrix `X` and target vector `y` are not empty before attempting the split.\n",
    "* **Train-Test Split:** Divide the dataset into training (80%) and testing (20%) sets using `train_test_split`.\n",
    "    * `stratify=y` is used to ensure that the proportion of winner classes (0 and 1) is approximately the same in both the training and testing sets. This is important for reliable evaluation, especially if the classes are not perfectly balanced.\n",
    "    * `random_state=42` ensures that the split is the same every time the code is run, making the results reproducible.\n",
    "* **Cross-Validation Strategy:** Define the cross-validation method using `StratifiedKFold`.\n",
    "    * `n_splits=5` means the training data will be split into 5 folds for cross-validation during hyperparameter tuning (Grid Search).\n",
    "    * `shuffle=True` randomizes the data before splitting into folds.\n",
    "    * `random_state=42` ensures reproducibility of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X and y are not empty before splitting\n",
    "if X.empty or y.empty:\n",
    "    print(\"\\nError: Feature matrix X or target vector y is empty. Cannot perform train-test split.\")\n",
    "else:\n",
    "    # Split data into training and testing sets (e.g., 80% train, 20% test)\n",
    "    # Use stratify=y to maintain class proportions in train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,      # Hold out 20% for testing\n",
    "        random_state=42,    # For reproducibility\n",
    "        stratify=y          # Important for classification, especially if classes are imbalanced\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Set up cross-validation strategy\n",
    "    # Stratified K-Fold is good for classification to preserve class percentage in each fold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    print(f\"Using Stratified K-Fold with {cv.get_n_splits()} splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7c525",
   "metadata": {},
   "source": [
    "## SVM Pipeline and Expanded Parameter Grid Setup\n",
    "\n",
    "This section sets up the Support Vector Machine (SVM) modeling framework using pipelines and defines an expanded parameter grid for hyperparameter tuning. The steps include:\n",
    "\n",
    "* **Pipeline Creation:** Define two `Pipeline` objects (`pipeline_no_weights`, `pipeline_balanced`).\n",
    "    * Each pipeline includes a scaler step (initially `StandardScaler`, but will be replaced by GridSearchCV) and an `SVC` (Support Vector Classifier) step.\n",
    "    * `probability=True` is set to enable probability estimates needed for ROC AUC calculation.\n",
    "    * `random_state=42` ensures reproducibility.\n",
    "    * One pipeline uses `class_weight=None` (default), and the other uses `class_weight='balanced'` to address potential class imbalance.\n",
    "    * `max_iter=10000` is added as a safeguard against potential convergence issues with certain parameter combinations.\n",
    "* **Expanded Parameter Grid (`param_grid`):** Define a list of dictionaries specifying the hyperparameters to search over.\n",
    "    * Tests both `StandardScaler` and `RobustScaler` for the 'scaler' step.\n",
    "    * Tests both `'linear'` and `'rbf'` kernels for the 'svm__kernel' step.\n",
    "    * Defines ranges for `'svm__C'` (regularization parameter) using `np.logspace` for both kernels.\n",
    "    * Defines ranges for `'svm__gamma'` (kernel coefficient for RBF) using `np.logspace`.\n",
    "* **Test Grid (Commented Out):** Includes a much smaller grid (`param_grid_test`) useful for quick debugging or testing the workflow.\n",
    "* **Grid Calculation:** Calculates and prints the total number of parameter combinations and model fits that will be performed during the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for SVM. \n",
    "# The 'scaler' step is a placeholder that GridSearchCV will fill.\n",
    "# We will compare performance with and without class weighting.\n",
    "\n",
    "pipeline_no_weights = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Default scaler, will be replaced by grid search\n",
    "    # Added max_iter to prevent potential infinite loops on non-converging fits\n",
    "    ('svm', SVC(probability=True, random_state=42, class_weight=None, max_iter=10000)) \n",
    "])\n",
    "\n",
    "pipeline_balanced = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Default scaler, will be replaced by grid search\n",
    "    # Added max_iter to prevent potential infinite loops on non-converging fits\n",
    "    ('svm', SVC(probability=True, random_state=42, class_weight='balanced', max_iter=10000)) \n",
    "])\n",
    "\n",
    "# --- Expanded Parameter Grid --- \n",
    "# Define ranges for C and gamma (for RBF kernel)\n",
    "# Use logspace for C and gamma as optimal values often span orders of magnitude\n",
    "param_grid = [\n",
    "    # Grid for Linear Kernel\n",
    "    {\n",
    "        'scaler': [StandardScaler(), RobustScaler()], # Test both scalers\n",
    "        'svm__kernel': ['linear'],\n",
    "        'svm__C': np.logspace(-2, 2, 5) # e.g., [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    # Grid for RBF Kernel\n",
    "    {\n",
    "        'scaler': [StandardScaler(), RobustScaler()], # Test both scalers\n",
    "        'svm__kernel': ['rbf'],\n",
    "        'svm__C': np.logspace(-2, 2, 5), # e.g., [0.01, 0.1, 1, 10, 100]\n",
    "        'svm__gamma': np.logspace(-3, 1, 5) # e.g., [0.001, 0.01, 0.1, 1, 10]\n",
    "        # Can also add 'scale' and 'auto' for gamma if desired\n",
    "        # 'svm__gamma': ['scale', 'auto'] + np.logspace(-3, 1, 5).tolist()\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Simplified Parameter Grid (for testing/debugging) ---\n",
    "# Uncomment this grid and comment out the one above to run faster\n",
    "# param_grid_test = [\n",
    "#     {\n",
    "#         'scaler': [StandardScaler()],\n",
    "#         'svm__kernel': ['linear'],\n",
    "#         'svm__C': [1] # Just one value\n",
    "#     },\n",
    "#     {\n",
    "#         'scaler': [StandardScaler()],\n",
    "#         'svm__kernel': ['rbf'],\n",
    "#         'svm__C': [10], # Test a potentially better C\n",
    "#         'svm__gamma': [0.1] # Test a potentially better gamma\n",
    "#     }\n",
    "# ]\n",
    "# Use param_grid_test in GridSearchCV below if testing\n",
    "# current_param_grid = param_grid_test \n",
    "current_param_grid = param_grid # Use the full grid by default\n",
    "\n",
    "print(\"Pipelines and Parameter Grid defined.\")\n",
    "print(f\"Using {'Full' if current_param_grid == param_grid else 'Simplified Test'} Parameter Grid.\")\n",
    "\n",
    "# Calculate expected number of fits for the current grid\n",
    "n_scalers = len(current_param_grid[0]['scaler'])\n",
    "n_linear_configs = len(current_param_grid[0]['svm__C'])\n",
    "n_rbf_configs = len(current_param_grid[1]['svm__C']) * len(current_param_grid[1]['svm__gamma'])\n",
    "total_combinations = n_scalers * (n_linear_configs + n_rbf_configs)\n",
    "n_folds = cv.get_n_splits()\n",
    "\n",
    "print(f\"Scalers to test: {n_scalers}\")\n",
    "print(f\"Linear SVM configurations: {n_scalers * n_linear_configs}\")\n",
    "print(f\"RBF SVM configurations: {n_scalers * n_rbf_configs}\")\n",
    "print(f\"Total parameter combinations to check per weighting approach: {total_combinations}\")\n",
    "print(f\"Total fits per grid search (combinations * folds): {total_combinations * n_folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b807639",
   "metadata": {},
   "source": [
    "## Grid Search Execution\n",
    "\n",
    "This cell performs the hyperparameter tuning using `GridSearchCV`. It systematically evaluates different combinations of scalers, kernels, and SVM parameters defined in the `param_grid`. The process involves:\n",
    "\n",
    "* **Data Check:** Verifying that the training data (`X_train`, `y_train`) is available and not empty.\n",
    "* **Grid Search with Balanced Weights:** Running `GridSearchCV` using `pipeline_balanced`.\n",
    "    * Uses the specified parameter grid (`current_param_grid`), cross-validation strategy (`cv`), and F1 scoring.\n",
    "    * `n_jobs=-1` utilizes all available CPU cores for parallel processing.\n",
    "    * `verbose=2` provides detailed output during the fitting process.\n",
    "    * Records the time taken for the search.\n",
    "* **Grid Search with No Weights:** Running `GridSearchCV` using `pipeline_no_weights` with the same settings.\n",
    "    * Records the time taken for this search.\n",
    "* **Output Results:** Printing the best parameters and the corresponding best cross-validation F1 score found for both the balanced and no-weights approaches.\n",
    "* **Select Best Model:** Comparing the best scores from the two approaches and selecting the overall best pipeline (`best_svm_pipeline`) based on the higher cross-validation F1 score. The training time associated with the best approach is also stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if X_train and y_train are valid before proceeding\n",
    "if 'X_train' not in locals() or 'y_train' not in locals() or X_train.empty or y_train.empty:\n",
    "    print(\"\\nError: Training data (X_train, y_train) not available or empty. Cannot perform grid search.\")\n",
    "else:\n",
    "    # Double check for NaNs/Infs right before fitting\n",
    "    print(f\"\\nNaNs in X_train before grid search: {X_train.isnull().values.any()}\")\n",
    "    print(f\"Infinities in X_train before grid search: {np.any(np.isinf(X_train.values))}\")\n",
    "    \n",
    "    # --- Grid search for balanced weights --- \n",
    "    print(\"\\nPerforming grid search with 'balanced' class weights...\")\n",
    "    grid_search_balanced = GridSearchCV(\n",
    "        pipeline_balanced, \n",
    "        current_param_grid, # Use the currently selected grid (full or test)     \n",
    "        cv=cv,           \n",
    "        scoring='f1',    # Use F1 score as the evaluation metric\n",
    "        n_jobs=-1,       # Use all available CPU cores\n",
    "        verbose=2        # Increased verbosity to show more progress\n",
    "    )\n",
    "    \n",
    "    start_time_balanced = time.time()\n",
    "    grid_search_balanced.fit(X_train, y_train)\n",
    "    grid_search_time_balanced = time.time() - start_time_balanced\n",
    "    print(f\"Grid search (balanced) completed in {grid_search_time_balanced:.2f} seconds.\")\n",
    "    \n",
    "    # --- Grid search with NO class weights --- \n",
    "    print(\"\\nPerforming grid search with NO class weights...\")\n",
    "    grid_search_no_weights = GridSearchCV(\n",
    "        pipeline_no_weights, \n",
    "        current_param_grid, # Use the currently selected grid (full or test)\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=2        # Increased verbosity\n",
    "    )\n",
    "    start_time_nw = time.time()\n",
    "    grid_search_no_weights.fit(X_train, y_train)\n",
    "    grid_search_time_no_weights = time.time() - start_time_nw\n",
    "    print(f\"Grid search (no weights) completed in {grid_search_time_no_weights:.2f} seconds.\")\n",
    "    \n",
    "    # --- Output Best Results --- \n",
    "    print(f\"\\n--- Results (Balanced Weights) ---\")\n",
    "    print(f\"Best parameters: {grid_search_balanced.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score: {grid_search_balanced.best_score_:.4f}\")\n",
    "    \n",
    "    print(f\"\\n--- Results (No Weights) ---\")\n",
    "    print(f\"Best parameters: {grid_search_no_weights.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score: {grid_search_no_weights.best_score_:.4f}\")\n",
    "    \n",
    "    # --- Select the Overall Best Model --- \n",
    "    if grid_search_no_weights.best_score_ >= grid_search_balanced.best_score_:\n",
    "        print(\"\\nSelecting best model from 'No weights' grid search.\")\n",
    "        best_svm_pipeline = grid_search_no_weights.best_estimator_\n",
    "        best_overall_score = grid_search_no_weights.best_score_\n",
    "        grid_search_time = grid_search_time_no_weights # Store time for comparison\n",
    "    else:\n",
    "        print(\"\\nSelecting best model from 'Balanced weights' grid search.\")\n",
    "        best_svm_pipeline = grid_search_balanced.best_estimator_\n",
    "        best_overall_score = grid_search_balanced.best_score_\n",
    "        grid_search_time = grid_search_time_balanced # Store time for comparison\n",
    "        \n",
    "    print(f\"Overall best cross-validation F1 score: {best_overall_score:.4f}\")\n",
    "    print(f\"Parameters of overall best model: {best_svm_pipeline.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29214f2e",
   "metadata": {},
   "source": [
    "## Final Model Evaluation on Test Set\n",
    "\n",
    "After identifying the best hyperparameters through grid search and cross-validation, this cell evaluates the performance of the final, optimized SVM pipeline (`best_svm_pipeline`) on the held-out test set (`X_test`, `y_test`). This provides an unbiased estimate of the model's generalization ability on unseen data. The steps are:\n",
    "\n",
    "* **Data Check:** Ensure the best pipeline and test data are available.\n",
    "* **Prediction:** Use the best pipeline to make predictions (`y_pred_test`) and predict probabilities (`y_prob_test`) on the test set.\n",
    "* **Calculate Metrics:** Compute various standard classification metrics:\n",
    "    * Accuracy\n",
    "    * Confusion Matrix (visualized with a heatmap)\n",
    "    * Sensitivity (Recall / True Positive Rate)\n",
    "    * Specificity (True Negative Rate)\n",
    "    * Precision\n",
    "    * F1 Score\n",
    "    * Full Classification Report (includes precision, recall, f1-score per class)\n",
    "    * ROC AUC Score\n",
    "* **Plot ROC Curve:** Visualize the trade-off between the True Positive Rate and False Positive Rate for the final model on the test set.\n",
    "* **Save Plots:** Save the generated confusion matrix and ROC curve plots as image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the overall best pipeline on the held-out test set\n",
    "\n",
    "# Check if best_svm_pipeline exists and test data is valid\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Grid search might have failed or not run.\")\n",
    "elif 'X_test' not in locals() or 'y_test' not in locals() or X_test.empty or y_test.empty:\n",
    "     print(\"\\nError: Test data (X_test, y_test) not available or empty. Cannot evaluate model.\")\n",
    "else:\n",
    "    print(\"\\nEvaluating final selected model on the test set...\")\n",
    "    y_pred_test = best_svm_pipeline.predict(X_test)\n",
    "    y_prob_test = best_svm_pipeline.predict_proba(X_test)[:, 1] # Probabilities for the positive class (class 1)\n",
    "    \n",
    "    # --- Print Evaluation Metrics --- \n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Confusion Matrix:\")\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    # Plot confusion matrix for better visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Loss (0)', 'Predicted Win (1)'], \n",
    "                yticklabels=['Actual Loss (0)', 'Actual Win (1)'])\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Test Set Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_test_confusion_matrix.png') # Save the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate detailed metrics from confusion matrix\n",
    "    tn, fp, fn, tp = cm_test.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0 # Same as Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    print(\"\\nTest Set Detailed Metrics:\")\n",
    "    print(f\"Sensitivity (Recall / True Positive Rate): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    # --- ROC AUC Score and Curve --- \n",
    "    roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "    print(f\"\\nTest Set ROC AUC Score: {roc_auc_test:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob_test)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_test:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Diagonal line for random guessing\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Test Set Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('svm_test_roc_curve.png') # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3548c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "This section analyzes which features were most influential for the best SVM model's predictions. The approach depends on the kernel type determined by the grid search:\n",
    "\n",
    "* **Check Kernel Type:** Identify the kernel (`linear` or `rbf`) used by the `best_svm_pipeline`.\n",
    "* **Linear Kernel:**\n",
    "    * If the kernel is linear, extract the model coefficients (`.coef_`).\n",
    "    * Create a DataFrame showing each feature and its corresponding coefficient.\n",
    "    * Rank features by the absolute value of their coefficients (larger magnitude implies greater importance).\n",
    "    * Plot the top 10 most important features based on absolute coefficient values.\n",
    "* **Non-Linear Kernel (e.g., RBF):**\n",
    "    * If the kernel is non-linear, coefficients are not directly interpretable.\n",
    "    * Calculate feature importance using `permutation_importance` on the test set. This method measures the decrease in model score (F1 score, in this case) when a single feature's values are randomly shuffled.\n",
    "    * Organize the mean importance scores and standard deviations into a DataFrame.\n",
    "    * Plot the top 10 most important features based on mean permutation importance, including error bars representing the standard deviation across repeats.\n",
    "* **Save Plot:** Save the generated feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b158a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance --- \n",
    "# Check the kernel type of the best SVM model found by grid search\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot determine feature importance.\")\n",
    "else:\n",
    "    best_kernel = best_svm_pipeline.named_steps['svm'].kernel\n",
    "    print(f\"\\nBest model kernel type: {best_kernel}\")\n",
    "\n",
    "    # Get the scaler from the best pipeline to scale the full dataset for importance calculation\n",
    "    # Important: Use the scaler fitted on the TRAINING data for consistency\n",
    "    # Refit the best scaler on the full training set before using it here\n",
    "    best_scaler_obj = best_svm_pipeline.named_steps['scaler']\n",
    "    # It's generally better practice to calculate permutation importance on the test set\n",
    "    # or a held-out validation set to avoid overfitting bias in importance scores.\n",
    "    # We'll use the test set here.\n",
    "    \n",
    "    # Scale the test data using the scaler from the pipeline (already fitted on train)\n",
    "    # Note: The pipeline handles scaling automatically when predicting, but for permutation \n",
    "    # importance outside the pipeline context, we might need scaled data depending on the approach.\n",
    "    # However, permutation_importance can take the estimator directly.\n",
    "\n",
    "    if best_kernel == 'linear':\n",
    "        print(\"Calculating feature importance using Linear SVM coefficients...\")\n",
    "        # Coefficients are available directly for the linear kernel\n",
    "        coefficients = best_svm_pipeline.named_steps['svm'].coef_[0]\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns, # Use columns from training data\n",
    "            'Coefficient': coefficients\n",
    "        })\n",
    "        # Use absolute value for ranking importance magnitude\n",
    "        feature_importance_df['Abs_Coefficient'] = abs(feature_importance_df['Coefficient'])\n",
    "        feature_importance_df = feature_importance_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance (Top 10 based on Linear SVM Coefficients):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.barplot(x='Abs_Coefficient', y='Feature', data=feature_importance_df.head(10), palette='viridis')\n",
    "        plt.title('Top 10 Features by Absolute Coefficient (Linear SVM)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('svm_linear_feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "    else: # For non-linear kernels like 'rbf'\n",
    "        print(\"\\nCalculating feature importance using Permutation Importance (on test set)...\")\n",
    "        # Use permutation importance on the test set for a less biased estimate\n",
    "        perm_importance = permutation_importance(\n",
    "            best_svm_pipeline, \n",
    "            X_test, \n",
    "            y_test, \n",
    "            n_repeats=10,       # Number of times to permute a feature\n",
    "            random_state=42,\n",
    "            scoring='f1',       # Use the same scoring metric as grid search\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Organize results into a DataFrame\n",
    "        sorted_idx = perm_importance.importances_mean.argsort()[::-1] # Get indices sorted by importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns[sorted_idx],\n",
    "            'Importance': perm_importance.importances_mean[sorted_idx],\n",
    "            'Std Dev': perm_importance.importances_std[sorted_idx]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nFeature Importance (Top 10 based on Permutation Importance):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        # Plot permutation importance\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10), \n",
    "                    palette='viridis', xerr=feature_importance_df['Std Dev'].head(10))\n",
    "        plt.title(f'Top 10 Features by Permutation Importance ({best_kernel.upper()} SVM)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'svm_{best_kernel}_feature_importance.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d7886",
   "metadata": {},
   "source": [
    "## Cross-Validation and Baseline Comparison\n",
    "\n",
    "This section provides further evaluation and context for the optimized SVM model. It includes:\n",
    "\n",
    "* **Cross-Validation on Full Dataset:**\n",
    "    * Performs k-fold cross-validation (using the same `StratifiedKFold` strategy as the grid search) on the *entire* dataset (`X`, `y`) using the `best_svm_pipeline`.\n",
    "    * Calculates the F1 score for each fold.\n",
    "    * Reports the mean and standard deviation of the cross-validation F1 scores. This gives an estimate of the model's stability and expected performance on different subsets of the data.\n",
    "* **Baseline Model Comparison:**\n",
    "    * Defines two baseline SVM models within pipelines: one with a linear kernel and default parameters, and one with an RBF kernel and default parameters (both using `StandardScaler`).\n",
    "    * Trains these baseline models on the training set (`X_train`, `y_train`).\n",
    "    * Evaluates the baselines on the test set (`X_test`, `y_test`), calculating Accuracy, F1 Score, Precision, Recall, and Training Time.\n",
    "    * Combines the results of the baseline models and the previously evaluated `Optimized SVM` into a DataFrame.\n",
    "    * Prints the comparison table.\n",
    "    * Generates bar plots comparing the performance metrics (Accuracy, F1, Precision, Recall) and the training times of the optimized model versus the baselines.\n",
    "    * Saves the comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-validation for Reliability Estimation --- \n",
    "# Perform cross-validation on the *entire* dataset using the *best found pipeline*\n",
    "# This gives an estimate of how the model might perform on average on new, unseen data splits\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot perform cross-validation.\")\n",
    "elif X.empty or y.empty:\n",
    "     print(\"\\nError: Full dataset (X, y) not available or empty. Cannot perform cross-validation.\")\n",
    "else:\n",
    "    print(\"\\nPerforming cross-validation on the full dataset with the best pipeline...\")\n",
    "    # Use the same CV strategy as in grid search\n",
    "    cv_scores = cross_val_score(best_svm_pipeline, X, y, cv=cv, scoring='f1', n_jobs=-1)\n",
    "    print(f\"\\nCross-validation F1 scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {cv_scores.mean():.4f}  {cv_scores.std():.4f}\")\n",
    "\n",
    "# --- Compare with Baseline Models --- \n",
    "print(\"\\nComparing with baseline models (untuned, default parameters)...\")\n",
    "\n",
    "# Baseline Linear SVM (using default C=1)\n",
    "baseline_linear = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Use standard scaler for baseline\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Baseline RBF SVM (using default C=1, gamma='scale')\n",
    "baseline_rbf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Dictionary to store results\n",
    "baseline_results = {}\n",
    "baseline_models = {\n",
    "    'Baseline Linear SVM': baseline_linear,\n",
    "    'Baseline RBF SVM': baseline_rbf\n",
    "}\n",
    "\n",
    "# Check if training data is valid\n",
    "if 'X_train' not in locals() or 'y_train' not in locals() or X_train.empty or y_train.empty:\n",
    "    print(\"\\nError: Training data not available. Cannot train baseline models.\")\n",
    "elif 'X_test' not in locals() or 'y_test' not in locals() or X_test.empty or y_test.empty:\n",
    "    print(\"\\nError: Test data not available. Cannot evaluate baseline models.\")\n",
    "else:\n",
    "    # Train and evaluate baseline models\n",
    "    for name, model in baseline_models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Evaluating {name} on test set...\")\n",
    "        y_pred_base = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred_base)\n",
    "        report = classification_report(y_test, y_pred_base, output_dict=True, zero_division=0)\n",
    "        \n",
    "        baseline_results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': report['weighted avg']['f1-score'],\n",
    "            'Precision': report['weighted avg']['precision'],\n",
    "            'Recall': report['weighted avg']['recall'],\n",
    "            'Training Time (s)': train_time\n",
    "        }\n",
    "\n",
    "    # Add the optimized model's results for comparison\n",
    "    if 'best_svm_pipeline' in locals():\n",
    "        baseline_results['Optimized SVM'] = {\n",
    "            'Accuracy': test_accuracy, # Calculated in the previous evaluation cell\n",
    "            'F1 Score': f1, # Calculated in the previous evaluation cell\n",
    "            'Precision': precision, # Calculated in the previous evaluation cell\n",
    "            'Recall': sensitivity, # Calculated in the previous evaluation cell\n",
    "            'Training Time (s)': grid_search_time # Time for the winning grid search\n",
    "        }\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_comparison_df = pd.DataFrame(baseline_results).T\n",
    "    print(\"\\n--- Model Comparison Table ---\")\n",
    "    print(results_comparison_df)\n",
    "    \n",
    "    # Create comparison bar chart for performance metrics\n",
    "    plt.style.use('seaborn-v0_8-talk') # Use a style for better visuals\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    results_comparison_df[['Accuracy', 'F1 Score', 'Precision', 'Recall']].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Model Performance Comparison (Test Set)')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    ax.legend(title='Metric', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_model_comparison_performance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison bar chart for training time\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    results_comparison_df[['Training Time (s)']].plot(kind='bar', ax=ax, legend=False, color='skyblue')\n",
    "    ax.set_title('Model Training Time Comparison')\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_model_comparison_time.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7377f",
   "metadata": {},
   "source": [
    "## Learning Curve Analysis\n",
    "\n",
    "Finally, this cell generates a learning curve for the best SVM pipeline. Learning curves help diagnose model performance issues related to bias and variance, and indicate whether collecting more data might be beneficial. The steps are:\n",
    "\n",
    "* **Data Check:** Ensure the best pipeline and the full dataset (`X`, `y`) are available.\n",
    "* **Generate Learning Curve Data:** Use the `learning_curve` function from scikit-learn.\n",
    "    * Pass the `best_svm_pipeline`, the full dataset (`X`, `y`), and the cross-validation strategy (`cv`).\n",
    "    * Specify `train_sizes` (e.g., 10 points from 10% to 100% of the training data) to evaluate performance on different dataset sizes.\n",
    "    * Use F1 score for evaluation.\n",
    "    * This function returns the training sizes used, and the corresponding training scores and cross-validation scores for each size across the different CV folds.\n",
    "* **Calculate Statistics:** Compute the mean and standard deviation of the training and cross-validation scores across the folds for each training size.\n",
    "* **Plot Learning Curve:**\n",
    "    * Plot the mean training score and mean cross-validation score against the number of training examples.\n",
    "    * Include shaded areas representing  one standard deviation around the mean scores (using `fill_between`) to show variability.\n",
    "    * Add title, labels, legend, and grid for clarity.\n",
    "* **Save Plot:** Save the generated learning curve plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ee18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning Curve --- \n",
    "# Generate learning curves for the best model to understand bias/variance\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot generate learning curve.\")\n",
    "elif X.empty or y.empty:\n",
    "     print(\"\\nError: Full dataset (X, y) not available or empty. Cannot generate learning curve.\")\n",
    "else:\n",
    "    print(\"\\nGenerating learning curves for the best SVM pipeline...\")\n",
    "    train_sizes_abs, train_scores, test_scores = learning_curve(\n",
    "        best_svm_pipeline, # The best estimator pipeline found\n",
    "        X,                # Full feature set\n",
    "        y,                # Full target variable\n",
    "        cv=cv,            # Use the same cross-validation strategy\n",
    "        n_jobs=-1,        # Use all cores\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), # Test 10 sizes from 10% to 100%\n",
    "        scoring='f1',     # Evaluate using F1 score\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Calculate mean and std deviation for plotting\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Plot the learning curve\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Use a clean style\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes_abs, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes_abs, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes_abs, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.title(\"Learning Curve (SVM)\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_learning_curve.png') # Save the plot\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSVM analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
