{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ff78d1",
   "metadata": {},
   "source": [
    "# Final Project: Sporting Game Outcome Prediction\n",
    "## Data Loading and Processing\n",
    "\n",
    "This initial section focuses on setting up the environment and defining functions to load and transform the raw play-by-play data into a usable format for game outcome prediction. We will:\n",
    "\n",
    "* **Import Libraries:** Bring in essential libraries like pandas for data manipulation, numpy for numerical operations, matplotlib/seaborn for visualization, and various modules from scikit-learn for modeling and evaluation.\n",
    "* **Define `load_and_process_data` Function:** Create a function to handle a single CSV file. This function will:\n",
    "    * Read the play-by-play data.\n",
    "    * Identify unique teams and attempt robust home/away team assignment (checking for `IsHome` or similar columns, falling back to approximation if needed).\n",
    "    * Iterate through each game, accumulating team statistics (yards, attempts, turnovers, penalties, scores, etc.) by processing individual plays.\n",
    "    * Calculate game-level differential features (e.g., `yards_diff`, `turnovers_diff`, `ypa_diff`) which often provide strong predictive signals.\n",
    "    * Determine the game winner based on simplified score calculation (to be used as the target variable).\n",
    "    * Return a pandas DataFrame where each row represents a single game with its aggregated stats and differentials.\n",
    "* **Define `combine_data_multiple_years` Function:** Create a helper function to apply `load_and_process_data` to multiple yearly CSV files and concatenate the results into a single DataFrame.\n",
    "* **Handle Warnings:** Suppress convergence and user warnings for cleaner output during potentially long computations like grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler # Include both scalers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance # For non-linear feature importance\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Ignore convergence warnings for cleaner output during grid search\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Ignore potential user warnings from libraries\n",
    "\n",
    "# Function to load and process a single year of data\n",
    "def load_and_process_data(filepath):\n",
    "    \"\"\"\n",
    "    Load and process a single year of play-by-play data.\n",
    "    Aggregates statistics to the game level and calculates differentials.\n",
    "    Returns a DataFrame with game-level aggregated statistics.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    try:\n",
    "        # low_memory=False can help pandas infer types correctly on mixed-type columns\n",
    "        df = pd.read_csv(filepath, low_memory=False) \n",
    "        print(f\" Data shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame if file not found\n",
    "\n",
    "    # Check for necessary columns early to ensure data integrity\n",
    "    required_cols = ['GameId', 'OffenseTeam', 'DefenseTeam', 'Yards', 'IsRush', 'IsPass', \n",
    "                     'IsSack', 'IsInterception', 'IsFumble', 'IsPenalty', 'PenaltyTeam', \n",
    "                     'PenaltyYards', 'IsTouchdown', 'PlayType', 'Description', 'Down', 'SeriesFirstDown']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: Missing required columns in {filepath}\")\n",
    "        missing = [col for col in required_cols if col not in df.columns]\n",
    "        print(f\"Missing columns: {missing}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # --- Home/Away Team Identification --- \n",
    "    # Attempt to identify home team more robustly using common column names\n",
    "    home_col = None\n",
    "    if 'IsHome' in df.columns:\n",
    "        home_col = 'IsHome'\n",
    "    # Add elif blocks here to check for other potential columns like 'HomeTeam' or 'GameLocation'\n",
    "    # based on exploration of the specific dataset structure.\n",
    "    \n",
    "    # Group by game to create game-level features\n",
    "    game_stats = {}\n",
    "\n",
    "    # Iterate through each unique game\n",
    "    for game_id in df['GameId'].unique():\n",
    "        # Use .copy() to avoid SettingWithCopyWarning when modifying game_df later\n",
    "        game_df = df[df['GameId'] == game_id].copy() \n",
    "\n",
    "        # Get unique teams involved in the game, handling potential NaN/empty strings\n",
    "        teams = set(game_df['OffenseTeam'].dropna().unique()) | set(game_df['DefenseTeam'].dropna().unique())\n",
    "        teams = [team for team in teams if isinstance(team, str) and team.strip()]\n",
    "\n",
    "        # Skip processing this game if we don't have exactly 2 valid team identifiers\n",
    "        if len(teams) != 2:\n",
    "            continue\n",
    "            \n",
    "        team1, team2 = teams[0], teams[1]\n",
    "        home_team, away_team = None, None\n",
    "\n",
    "        # Determine Home/Away based on identified column or approximation\n",
    "        if home_col == 'IsHome':\n",
    "            # Find the first play where IsHome is defined for team1 or team2 to assign roles\n",
    "            home_play = game_df[((game_df['OffenseTeam'] == team1) | (game_df['DefenseTeam'] == team1)) & game_df['IsHome'].notna()].iloc[0] if not game_df[((game_df['OffenseTeam'] == team1) | (game_df['DefenseTeam'] == team1)) & game_df['IsHome'].notna()].empty else None\n",
    "            if home_play is not None:\n",
    "                if home_play['IsHome'] == 1:\n",
    "                    home_team, away_team = team1, team2\n",
    "                else:\n",
    "                    home_team, away_team = team2, team1\n",
    "        \n",
    "        # If robust identification failed (no suitable column found or data missing), use approximation\n",
    "        if home_team is None or away_team is None:\n",
    "            # This approximation assumes the first team encountered is 'home'\n",
    "            home_team, away_team = team1, team2 \n",
    "\n",
    "        # Initialize dictionaries to store aggregated stats for home and away teams\n",
    "        home_stats = {\n",
    "            'total_yards': 0, 'pass_yards': 0, 'rush_yards': 0,\n",
    "            'pass_attempts': 0, 'rush_attempts': 0, 'turnovers': 0,\n",
    "            'sacks_taken': 0, 'penalties': 0, 'penalty_yards': 0,\n",
    "            'touchdowns': 0, 'field_goals': 0,\n",
    "            'third_down_attempts': 0, 'third_down_conversions': 0,\n",
    "            'fg_attempts': 0, 'punt_yards': 0\n",
    "        }\n",
    "        away_stats = home_stats.copy()\n",
    "\n",
    "        # Calculate stats for each team by iterating through plays within the game\n",
    "        for _, play in game_df.iterrows():\n",
    "            off_team = play['OffenseTeam']\n",
    "            # Skip plays with missing offense team identifier\n",
    "            if not isinstance(off_team, str) or not off_team.strip():\n",
    "                continue \n",
    "\n",
    "            # Determine if the current play's offense is the designated home team\n",
    "            is_home_offense = (off_team == home_team)\n",
    "            # Select the appropriate dictionary to update\n",
    "            current_team_stats = home_stats if is_home_offense else away_stats\n",
    "\n",
    "            # --- Accumulate Stats --- \n",
    "            # Use .get(col, 0) to safely access columns, defaulting to 0 if column or value is missing/NaN\n",
    "            yards = play.get('Yards', 0)\n",
    "            if pd.notna(yards):\n",
    "                current_team_stats['total_yards'] += yards\n",
    "                if play.get('IsRush', 0) == 1:\n",
    "                    current_team_stats['rush_yards'] += yards\n",
    "                    current_team_stats['rush_attempts'] += 1\n",
    "                elif play.get('IsPass', 0) == 1:\n",
    "                    current_team_stats['pass_yards'] += yards\n",
    "                    current_team_stats['pass_attempts'] += 1\n",
    "            \n",
    "            # Sacks are recorded against the offense\n",
    "            if play.get('IsSack', 0) == 1:\n",
    "                current_team_stats['sacks_taken'] += 1\n",
    "                # Ensure pass attempt is counted for sacks if not already marked as IsPass=1\n",
    "                if play.get('IsPass', 0) != 1:\n",
    "                     current_team_stats['pass_attempts'] += 1\n",
    "            \n",
    "            # Turnovers (Interceptions or Fumbles)\n",
    "            if play.get('IsInterception', 0) == 1 or play.get('IsFumble', 0) == 1:\n",
    "                current_team_stats['turnovers'] += 1\n",
    "                \n",
    "            # Penalties (only count if against the current offensive team)\n",
    "            if play.get('IsPenalty', 0) == 1 and pd.notna(play.get('PenaltyTeam')) and play['PenaltyTeam'] == off_team:\n",
    "                current_team_stats['penalties'] += 1\n",
    "                penalty_yards = play.get('PenaltyYards', 0)\n",
    "                if pd.notna(penalty_yards):\n",
    "                     current_team_stats['penalty_yards'] += penalty_yards\n",
    "            \n",
    "            # Scoring Plays (ensure it's the offense scoring)\n",
    "            if play.get('IsTouchdown', 0) == 1 and off_team == play.get('OffenseTeam'): \n",
    "                current_team_stats['touchdowns'] += 1\n",
    "            if play.get('PlayType') == 'FIELD GOAL':\n",
    "                current_team_stats['fg_attempts'] += 1 # Count attempt\n",
    "                # Check description for success (this might need refinement based on specific data formats)\n",
    "                if 'IS GOOD' in str(play.get('Description', '')).upper():\n",
    "                    current_team_stats['field_goals'] += 1 # Count make\n",
    "            \n",
    "            # Third Down Attempts and Conversions\n",
    "            if play.get('Down') == 3:\n",
    "                current_team_stats['third_down_attempts'] += 1\n",
    "                if play.get('SeriesFirstDown', 0) == 1: # Check if the play resulted in a first down\n",
    "                    current_team_stats['third_down_conversions'] += 1\n",
    "            \n",
    "            # Punt Yards (assuming 'Yards' reflects net yardage, may need data validation)\n",
    "            if play.get('PlayType') == 'PUNT':\n",
    "                 punt_yards = play.get('Yards', 0)\n",
    "                 if pd.notna(punt_yards):\n",
    "                    current_team_stats['punt_yards'] += punt_yards\n",
    "        \n",
    "        # --- Calculate Game Outcome & Differentials --- \n",
    "        # Estimate final score (simplified: assumes 7 points for TD, 3 for FG)\n",
    "        home_score = home_stats['touchdowns'] * 7 + home_stats['field_goals'] * 3 \n",
    "        away_score = away_stats['touchdowns'] * 7 + away_stats['field_goals'] * 3 \n",
    "\n",
    "        # Determine winner (target variable: 1 if home team wins, 0 otherwise)\n",
    "        # Ties are treated as a loss for the home team (0) in this binary setup\n",
    "        winner = 1 if home_score > away_score else 0 \n",
    "        \n",
    "        # Small constant to prevent division by zero in ratio calculations\n",
    "        epsilon = 1e-6 \n",
    "\n",
    "        # Calculate efficiency differentials (Home - Away)\n",
    "        home_ypa = home_stats['pass_yards'] / (home_stats['pass_attempts'] + epsilon)\n",
    "        away_ypa = away_stats['pass_yards'] / (away_stats['pass_attempts'] + epsilon)\n",
    "        ypa_diff = home_ypa - away_ypa\n",
    "\n",
    "        home_yra = home_stats['rush_yards'] / (home_stats['rush_attempts'] + epsilon)\n",
    "        away_yra = away_stats['rush_yards'] / (away_stats['rush_attempts'] + epsilon)\n",
    "        yra_diff = home_yra - away_yra\n",
    "\n",
    "        home_3rd_conv_pct = home_stats['third_down_conversions'] / (home_stats['third_down_attempts'] + epsilon)\n",
    "        away_3rd_conv_pct = away_stats['third_down_conversions'] / (away_stats['third_down_attempts'] + epsilon)\n",
    "        third_down_conv_pct_diff = home_3rd_conv_pct - away_3rd_conv_pct\n",
    "        \n",
    "        # Calculate derived differentials (Home - Away)\n",
    "        home_total_attempts = home_stats['pass_attempts'] + home_stats['rush_attempts']\n",
    "        away_total_attempts = away_stats['pass_attempts'] + away_stats['rush_attempts']\n",
    "        total_attempts_diff = home_total_attempts - away_total_attempts\n",
    "        # Yards per play differential\n",
    "        yards_per_play_diff = (home_stats['total_yards'] / (home_total_attempts + epsilon)) - \\\n",
    "                              (away_stats['total_yards'] / (away_total_attempts + epsilon))\n",
    "        \n",
    "        # Rush-to-pass ratio differential\n",
    "        rush_pass_ratio_diff = (home_stats['rush_attempts'] / (home_stats['pass_attempts'] + epsilon)) - \\\n",
    "                               (away_stats['rush_attempts'] / (away_stats['pass_attempts'] + epsilon))\n",
    "\n",
    "        # Create dictionary of all calculated features for the game\n",
    "        # Note on differentials: Positive means home > away, except for turnovers, sacks, penalties where negative is better for home.\n",
    "        feature_dict = {\n",
    "            # Basic Differentials\n",
    "            'yards_diff': home_stats['total_yards'] - away_stats['total_yards'],\n",
    "            'pass_yards_diff': home_stats['pass_yards'] - away_stats['pass_yards'],\n",
    "            'rush_yards_diff': home_stats['rush_yards'] - away_stats['rush_yards'],\n",
    "            'turnovers_diff': home_stats['turnovers'] - away_stats['turnovers'], # Lower is better\n",
    "            'sacks_diff': home_stats['sacks_taken'] - away_stats['sacks_taken'], # Lower is better\n",
    "            'penalties_diff': home_stats['penalties'] - away_stats['penalties'], # Lower is better\n",
    "            'penalty_yards_diff': home_stats['penalty_yards'] - away_stats['penalty_yards'], # Lower is better\n",
    "            'pass_attempts_diff': home_stats['pass_attempts'] - away_stats['pass_attempts'],\n",
    "            'rush_attempts_diff': home_stats['rush_attempts'] - away_stats['rush_attempts'],\n",
    "            # Derived & Efficiency Differentials\n",
    "            'yards_per_play_diff': yards_per_play_diff,\n",
    "            'rush_pass_ratio_diff': rush_pass_ratio_diff,\n",
    "            'ypa_diff': ypa_diff, \n",
    "            'yra_diff': yra_diff, \n",
    "            'third_down_conv_pct_diff': third_down_conv_pct_diff,\n",
    "            # Other/Special Teams Differentials\n",
    "            'fg_attempts_diff': home_stats['fg_attempts'] - away_stats['fg_attempts'],\n",
    "            'punt_yards_diff': home_stats['punt_yards'] - away_stats['punt_yards'], \n",
    "            # --- Leaky Features (Included for potential analysis but dropped before training) ---\n",
    "            'touchdowns_diff': home_stats['touchdowns'] - away_stats['touchdowns'], \n",
    "            'field_goals_diff': home_stats['field_goals'] - away_stats['field_goals'], \n",
    "            'home_score': home_score,\n",
    "            'away_score': away_score,\n",
    "            # --- Target Variable ---\n",
    "            'winner': winner \n",
    "        }\n",
    "\n",
    "        # Store the features for this game\n",
    "        game_stats[game_id] = feature_dict\n",
    "\n",
    "    # Convert the dictionary of game stats into a DataFrame\n",
    "    games_df = pd.DataFrame.from_dict(game_stats, orient='index')\n",
    "    # Add GameId back as a column\n",
    "    games_df.reset_index(inplace=True)\n",
    "    games_df.rename(columns={'index': 'GameId'}, inplace=True)\n",
    "\n",
    "    print(f\" Extracted {len(games_df)} games from {filepath}.\")\n",
    "    return games_df\n",
    "\n",
    "# Function to combine multiple years of data\n",
    "def combine_data_multiple_years(filepaths):\n",
    "    \"\"\"\n",
    "    Load and combine processed game data from multiple CSV files.\n",
    "    \"\"\"\n",
    "    all_games_data = []\n",
    "    # Process each file and append the resulting DataFrame to a list\n",
    "    for filepath in filepaths:\n",
    "        year_data = load_and_process_data(filepath)\n",
    "        if not year_data.empty:\n",
    "            all_games_data.append(year_data)\n",
    "        else:\n",
    "            print(f\"Skipping empty DataFrame from {filepath}\")\n",
    "\n",
    "    # Combine all yearly DataFrames if any data was loaded\n",
    "    if not all_games_data:\n",
    "        print(\"No data loaded, returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    combined_data = pd.concat(all_games_data, ignore_index=True)\n",
    "    print(f\"\\nCombined data shape: {combined_data.shape}\")\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba5a85",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection\n",
    "\n",
    "This cell executes the data loading and initial feature preparation steps. It involves:\n",
    "\n",
    "* **Specifying Data Files:** Defining the list of CSV files containing the play-by-play data (adjust this list to include all relevant years).\n",
    "* **Loading Data:** Calling the previously defined functions (`load_and_process_data` or `combine_data_multiple_years`) to get the game-level DataFrame `games_df`.\n",
    "* **Handling Missing/Infinite Values:** Replacing any `inf` values (which might occur from divisions in feature calculation) with `NaN`, and then filling all remaining `NaN` values with 0. This ensures the data is clean for modeling. Filling with 0 is chosen here assuming a zero difference is a reasonable default if a calculation wasn't possible (e.g., due to zero attempts).\n",
    "* **Defining Feature Sets:** Explicitly listing columns that are identifiers (`GameId`), potentially leaky (features directly revealing the score or outcome like `touchdowns_diff`, `home_score`), or the target variable (`winner`).\n",
    "* **Separating X and y:** Creating the feature matrix `X` by selecting all columns from `games_df` except those identified to be dropped, and creating the target vector `y` containing only the `winner` column.\n",
    "* **Verification:** Printing the final list of features used for prediction, the total number of features, the class balance (proportion of home wins vs. losses) of the target variable, and performing a final check for any remaining `inf` or `NaN` values in the feature matrix `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths (Update this list with all your data files)\n",
    "data_filepaths = [\n",
    "    'pbp-2024.csv', \n",
    "    'pbp-2020.csv',\n",
    "    'pbp-2021.csv',\n",
    "    'pbp-2022.csv',\n",
    "    'pbp-2023.csv'\n",
    "]\n",
    "\n",
    "# Load and combine data from specified files\n",
    "if len(data_filepaths) == 1:\n",
    "    # If only one file, load it directly\n",
    "    games_df = load_and_process_data(data_filepaths[0])\n",
    "else:\n",
    "    # If multiple files, use the combine function\n",
    "    games_df = combine_data_multiple_years(data_filepaths)\n",
    "\n",
    "# --- Handle Potential Infinities and NaNs --- \n",
    "# Replace infinities resulting from calculations (e.g., division by zero) with NaN\n",
    "games_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill any remaining NaNs. Filling with 0 assumes a zero difference is a neutral default.\n",
    "# Consider more advanced imputation if NaNs are numerous or meaningful.\n",
    "if games_df.isnull().any().any():\n",
    "    print(\"\\nColumns with NaNs before filling:\")\n",
    "    print(games_df.isnull().sum()[games_df.isnull().sum() > 0])\n",
    "    games_df.fillna(0, inplace=True) # Fill NaNs with 0\n",
    "else:\n",
    "    print(\"\\nNo NaNs found in the aggregated DataFrame.\")\n",
    "\n",
    "# --- Separate Features (X) and Target (y) ---\n",
    "# Define leaky features - these reveal too much about the outcome and should not be used for training\n",
    "leaky_features = ['touchdowns_diff', 'field_goals_diff', 'home_score', 'away_score']\n",
    "# Define identifier columns to drop\n",
    "identifier_cols = ['GameId']\n",
    "# Combine all columns to drop from the feature set\n",
    "cols_to_drop = identifier_cols + leaky_features + ['winner'] # Also drop the target 'winner'\n",
    "\n",
    "# Define the final feature columns by taking all columns EXCEPT those to drop\n",
    "# Ensure 'winner' column exists before proceeding\n",
    "if 'winner' not in games_df.columns:\n",
    "    print(\"Error: 'winner' column not found in DataFrame. Cannot proceed.\")\n",
    "    X, y = pd.DataFrame(), pd.Series() # Assign empty structures to prevent further errors\n",
    "else:\n",
    "    # Select all columns not in the drop list as features\n",
    "    final_feature_cols = [col for col in games_df.columns if col not in cols_to_drop]\n",
    "    X = games_df[final_feature_cols].copy()\n",
    "    # Select the 'winner' column as the target variable\n",
    "    y = games_df['winner'].copy()\n",
    "\n",
    "    print(\"\\nFeatures used for prediction:\")\n",
    "    print(X.columns.tolist())\n",
    "    print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "    # Check class balance of the target variable 'winner'\n",
    "    print(\"\\nClass balance (winner):\")\n",
    "    class_balance = y.value_counts(normalize=True)\n",
    "    print(class_balance)\n",
    "\n",
    "    # Final verification for infinities/NaNs in the feature matrix X\n",
    "    print(f\"\\nInfinities remaining in X before split: {np.any(np.isinf(X.values))}\")\n",
    "    print(f\"NaNs remaining in X before split: {X.isnull().values.any()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59aadbf",
   "metadata": {},
   "source": [
    "## Train-Test Split and Cross-Validation Setup\n",
    "\n",
    "This cell prepares the data for model training and evaluation by splitting it and defining a cross-validation strategy. The key steps are:\n",
    "\n",
    "* **Check Data Validity:** Ensure the feature matrix `X` and target vector `y` are not empty before attempting the split.\n",
    "* **Train-Test Split:** Divide the dataset into training (80%) and testing (20%) sets using `train_test_split`.\n",
    "    * `stratify=y` is used to ensure that the proportion of winner classes (0 and 1) is approximately the same in both the training and testing sets. This is important for reliable evaluation, especially if the classes are not perfectly balanced.\n",
    "    * `random_state=42` ensures that the split is the same every time the code is run, making the results reproducible.\n",
    "* **Cross-Validation Strategy:** Define the cross-validation method using `StratifiedKFold`.\n",
    "    * `n_splits=5` means the training data will be split into 5 folds for cross-validation during hyperparameter tuning (Grid Search).\n",
    "    * `shuffle=True` randomizes the data before splitting into folds.\n",
    "    * `random_state=42` ensures reproducibility of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X and y are not empty before splitting\n",
    "if X.empty or y.empty:\n",
    "    print(\"\\nError: Feature matrix X or target vector y is empty. Cannot perform train-test split.\")\n",
    "else:\n",
    "    # Split data into training and testing sets (e.g., 80% train, 20% test)\n",
    "    # Use stratify=y to maintain class proportions in train and test sets, crucial for classification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,      # Reserve 20% of the data for the final test set\n",
    "        random_state=42,    # Ensures the split is the same each time (for reproducibility)\n",
    "        stratify=y          # Preserves the percentage of samples for each class in the split\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Set up the cross-validation strategy for hyperparameter tuning (GridSearch)\n",
    "    # Stratified K-Fold maintains class proportions within each fold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    print(f\"Using Stratified K-Fold with {cv.get_n_splits()} splits for cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7c525",
   "metadata": {},
   "source": [
    "## SVM Pipeline and Expanded Parameter Grid Setup\n",
    "\n",
    "This section sets up the Support Vector Machine (SVM) modeling framework using pipelines and defines an expanded parameter grid for hyperparameter tuning. The steps include:\n",
    "\n",
    "* **Pipeline Creation:** Define two `Pipeline` objects (`pipeline_no_weights`, `pipeline_balanced`).\n",
    "    * Each pipeline includes a scaler step (initially `StandardScaler`, but will be replaced by GridSearchCV) and an `SVC` (Support Vector Classifier) step.\n",
    "    * `probability=True` is set to enable probability estimates needed for ROC AUC calculation.\n",
    "    * `random_state=42` ensures reproducibility.\n",
    "    * One pipeline uses `class_weight=None` (default), and the other uses `class_weight='balanced'` to potentially improve performance on imbalanced datasets by adjusting class weights inversely proportional to class frequencies.\n",
    "    * `max_iter=10000` is added as a safeguard against potential convergence issues for complex fits, preventing the grid search from hanging indefinitely.\n",
    "* **Expanded Parameter Grid (`param_grid`):** Define a list of dictionaries specifying the hyperparameters to search over using `GridSearchCV`.\n",
    "    * Tests both `StandardScaler` (centers data with unit variance) and `RobustScaler` (uses median and IQR, less sensitive to outliers) for the 'scaler' step.\n",
    "    * Tests both `'linear'` (for linearly separable data) and `'rbf'` (Radial Basis Function, for non-linear data) kernels for the 'svm__kernel' step.\n",
    "    * Defines ranges for `'svm__C'` (regularization parameter; smaller C = stronger regularization) using `np.logspace` (logarithmic scale) for both kernels, as optimal C often spans orders of magnitude.\n",
    "    * Defines ranges for `'svm__gamma'` (kernel coefficient for RBF; influences the reach of a single training example) using `np.logspace`.\n",
    "* **Test Grid (Commented Out):** Includes a much smaller grid (`param_grid_test`) useful for quickly debugging the pipeline or testing the workflow without the long computation time of the full grid.\n",
    "* **Grid Calculation:** Calculates and prints the total number of parameter combinations and model fits that will be performed during the grid search for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b159a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for SVM. \n",
    "# Using pipelines ensures that scaling is applied correctly during cross-validation.\n",
    "# The 'scaler' step name matches the key in param_grid, allowing GridSearchCV to substitute scalers.\n",
    "\n",
    "pipeline_no_weights = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Default scaler, will be replaced by grid search\n",
    "    # Added max_iter to prevent potential infinite loops on non-converging fits\n",
    "    ('svm', SVC(probability=True, random_state=42, class_weight=None, max_iter=10000)) \n",
    "])\n",
    "\n",
    "pipeline_balanced = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Default scaler, will be replaced by grid search\n",
    "    # Added max_iter to prevent potential infinite loops on non-converging fits\n",
    "    ('svm', SVC(probability=True, random_state=42, class_weight='balanced', max_iter=10000)) \n",
    "])\n",
    "\n",
    "# --- Expanded Parameter Grid --- \n",
    "# Define parameter ranges for GridSearchCV\n",
    "# Using logspace is common for C and gamma as optimal values often span several orders of magnitude\n",
    "param_grid = [\n",
    "    # Grid for Linear Kernel\n",
    "    {\n",
    "        'scaler': [StandardScaler(), RobustScaler()], # Test both standard and robust scaling\n",
    "        'svm__kernel': ['linear'],\n",
    "        'svm__C': np.logspace(-2, 2, 5) # Test C values [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    # Grid for RBF Kernel\n",
    "    {\n",
    "        'scaler': [StandardScaler(), RobustScaler()], # Test both standard and robust scaling\n",
    "        'svm__kernel': ['rbf'],\n",
    "        'svm__C': np.logspace(-2, 2, 5), # Test C values [0.01, 0.1, 1, 10, 100]\n",
    "        'svm__gamma': np.logspace(-3, 1, 5) # Test gamma values [0.001, 0.01, 0.1, 1, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "current_param_grid = param_grid # Use the full grid by default\n",
    "\n",
    "print(\"Pipelines and Parameter Grid defined.\")\n",
    "print(f\"Using {'Full' if current_param_grid == param_grid else 'Simplified Test'} Parameter Grid.\")\n",
    "\n",
    "# Calculate expected number of fits for the current grid to estimate runtime\n",
    "n_scalers = len(current_param_grid[0]['scaler']) # Number of scalers tested\n",
    "n_linear_configs = len(current_param_grid[0]['svm__C']) # Number of C values for linear\n",
    "n_rbf_configs = len(current_param_grid[1]['svm__C']) * len(current_param_grid[1]['svm__gamma']) # C * gamma combinations for RBF\n",
    "total_combinations = n_scalers * (n_linear_configs + n_rbf_configs) # Total combinations per pipeline\n",
    "n_folds = cv.get_n_splits() # Number of cross-validation folds\n",
    "\n",
    "print(f\"Scalers to test: {n_scalers}\")\n",
    "print(f\"Linear SVM configurations (scaler * C): {n_scalers * n_linear_configs}\")\n",
    "print(f\"RBF SVM configurations (scaler * C * gamma): {n_scalers * n_rbf_configs}\")\n",
    "print(f\"Total parameter combinations to check per weighting approach: {total_combinations}\")\n",
    "print(f\"Total fits per grid search (combinations * folds): {total_combinations * n_folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b807639",
   "metadata": {},
   "source": [
    "## Grid Search Execution\n",
    "\n",
    "This cell performs the hyperparameter tuning using `GridSearchCV`. It systematically evaluates different combinations of scalers, kernels, and SVM parameters defined in the `param_grid`. The process involves:\n",
    "\n",
    "* **Data Check:** Verifying that the training data (`X_train`, `y_train`) is available and not empty, and checking for NaNs/Infs one last time.\n",
    "* **Grid Search with Balanced Weights:** Running `GridSearchCV` using `pipeline_balanced`.\n",
    "    * Uses the specified parameter grid (`current_param_grid`), cross-validation strategy (`cv`), and F1 scoring.\n",
    "    * `n_jobs=-1` utilizes all available CPU cores for parallel processing.\n",
    "    * `verbose=1` provides output during the fitting process (can increase for more detail).\n",
    "    * Records the time taken for the search.\n",
    "* **Grid Search with No Weights:** Running `GridSearchCV` using `pipeline_no_weights` with the same settings.\n",
    "    * Records the time taken for this search.\n",
    "* **Output Results:** Printing the best parameters and the corresponding best cross-validation F1 score found for both the balanced and no-weights approaches.\n",
    "* **Select Best Model:** Comparing the best scores from the two approaches and selecting the overall best pipeline (`best_svm_pipeline`) based on the higher cross-validation F1 score. The training time associated with the best approach is also stored for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if X_train and y_train are valid before proceeding\n",
    "if 'X_train' not in locals() or 'y_train' not in locals() or X_train.empty or y_train.empty:\n",
    "    print(\"\\nError: Training data (X_train, y_train) not available or empty. Cannot perform grid search.\")\n",
    "else:\n",
    "    # Double check for NaNs/Infs right before fitting, just in case\n",
    "    print(f\"\\nNaNs in X_train before grid search: {X_train.isnull().values.any()}\")\n",
    "    print(f\"Infinities in X_train before grid search: {np.any(np.isinf(X_train.values))}\")\n",
    "    \n",
    "    # --- Grid search for balanced weights --- \n",
    "    print(\"\\nPerforming grid search with 'balanced' class weights...\")\n",
    "    grid_search_balanced = GridSearchCV(\n",
    "        pipeline_balanced, \n",
    "        current_param_grid, # Use the currently selected grid (full or test)     \n",
    "        cv=cv,           \n",
    "        scoring='f1',    # Use F1 score as the evaluation metric for optimization\n",
    "        n_jobs=-1,       # Use all available CPU cores\n",
    "        verbose=1        # Show some progress updates (can increase to 2 for more detail)\n",
    "    )\n",
    "    \n",
    "    start_time_balanced = time.time()\n",
    "    grid_search_balanced.fit(X_train, y_train)\n",
    "    grid_search_time_balanced = time.time() - start_time_balanced\n",
    "    print(f\"Grid search (balanced) completed in {grid_search_time_balanced:.2f} seconds.\")\n",
    "    \n",
    "    # --- Grid search with NO class weights --- \n",
    "    print(\"\\nPerforming grid search with NO class weights...\")\n",
    "    grid_search_no_weights = GridSearchCV(\n",
    "        pipeline_no_weights, \n",
    "        current_param_grid, # Use the currently selected grid (full or test)\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1        # Show some progress updates\n",
    "    )\n",
    "    start_time_nw = time.time()\n",
    "    grid_search_no_weights.fit(X_train, y_train)\n",
    "    grid_search_time_no_weights = time.time() - start_time_nw\n",
    "    print(f\"Grid search (no weights) completed in {grid_search_time_no_weights:.2f} seconds.\")\n",
    "    \n",
    "    # --- Output Best Results --- \n",
    "    print(f\"\\n--- Results (Balanced Weights) ---\")\n",
    "    print(f\"Best parameters: {grid_search_balanced.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score: {grid_search_balanced.best_score_:.4f}\")\n",
    "    \n",
    "    print(f\"\\n--- Results (No Weights) ---\")\n",
    "    print(f\"Best parameters: {grid_search_no_weights.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score: {grid_search_no_weights.best_score_:.4f}\")\n",
    "    \n",
    "    # --- Select the Overall Best Model --- \n",
    "    # Choose the pipeline (balanced or no weights) that yielded the best CV F1 score\n",
    "    if grid_search_no_weights.best_score_ >= grid_search_balanced.best_score_:\n",
    "        print(\"\\nSelecting best model from 'No weights' grid search.\")\n",
    "        best_svm_pipeline = grid_search_no_weights.best_estimator_\n",
    "        best_overall_score = grid_search_no_weights.best_score_\n",
    "        grid_search_time = grid_search_time_no_weights # Store time for comparison\n",
    "    else:\n",
    "        print(\"\\nSelecting best model from 'Balanced weights' grid search.\")\n",
    "        best_svm_pipeline = grid_search_balanced.best_estimator_\n",
    "        best_overall_score = grid_search_balanced.best_score_\n",
    "        grid_search_time = grid_search_time_balanced # Store time for comparison\n",
    "        \n",
    "    print(f\"Overall best cross-validation F1 score: {best_overall_score:.4f}\")\n",
    "    # Print the full pipeline details of the selected best model\n",
    "    print(f\"\\nSelected best pipeline for evaluation: \\n{best_svm_pipeline}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29214f2e",
   "metadata": {},
   "source": [
    "## Final Model Evaluation on Test Set\n",
    "\n",
    "After identifying the best hyperparameters through grid search and cross-validation, this cell evaluates the performance of the final, optimized SVM pipeline (`best_svm_pipeline`) on the held-out test set (`X_test`, `y_test`). This provides an unbiased estimate of the model's generalization ability on unseen data. The steps are:\n",
    "\n",
    "* **Data Check:** Ensure the best pipeline and test data are available.\n",
    "* **Prediction:** Use the best pipeline to make predictions (`y_pred_test`) and predict probabilities (`y_prob_test`) on the test set.\n",
    "* **Calculate Metrics:** Compute various standard classification metrics:\n",
    "    * Accuracy\n",
    "    * Confusion Matrix (visualized with a heatmap)\n",
    "    * Sensitivity (Recall / True Positive Rate)\n",
    "    * Specificity (True Negative Rate)\n",
    "    * Precision\n",
    "    * F1 Score\n",
    "    * Full Classification Report (includes precision, recall, f1-score per class)\n",
    "    * ROC AUC Score\n",
    "* **Plot ROC Curve:** Visualize the trade-off between the True Positive Rate and False Positive Rate for the final model on the test set.\n",
    "* **Save Plots:** Save the generated confusion matrix and ROC curve plots as image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the overall best pipeline on the held-out test set\n",
    "\n",
    "# Check if best_svm_pipeline exists and test data is valid\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Grid search might have failed or not run.\")\n",
    "elif 'X_test' not in locals() or 'y_test' not in locals() or X_test.empty or y_test.empty:\n",
    "     print(\"\\nError: Test data (X_test, y_test) not available or empty. Cannot evaluate model.\")\n",
    "else:\n",
    "    print(\"\\nEvaluating final selected model on the test set...\")\n",
    "    # Predict class labels\n",
    "    y_pred_test = best_svm_pipeline.predict(X_test)\n",
    "    # Predict probabilities for the positive class (needed for ROC AUC)\n",
    "    y_prob_test = best_svm_pipeline.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "    # --- Print Evaluation Metrics --- \n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Confusion Matrix:\")\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    # Plot confusion matrix for better visualization\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted Loss (0)', 'Predicted Win (1)'], \n",
    "                yticklabels=['Actual Loss (0)', 'Actual Win (1)'])\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title('Test Set Confusion Matrix (Optimized SVM)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_test_confusion_matrix.png') # Save the plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate detailed metrics from confusion matrix elements\n",
    "    tn, fp, fn, tp = cm_test.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0 # Same as Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    print(\"\\nTest Set Detailed Metrics:\")\n",
    "    print(f\"Sensitivity (Recall / True Positive Rate): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Classification Report:\")\n",
    "    # Use zero_division=0 to prevent warnings if a class has no predictions\n",
    "    print(classification_report(y_test, y_pred_test, zero_division=0))\n",
    "    \n",
    "    # --- ROC AUC Score and Curve --- \n",
    "    roc_auc_test = roc_auc_score(y_test, y_prob_test)\n",
    "    print(f\"\\nTest Set ROC AUC Score: {roc_auc_test:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob_test)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Diagonal line for random guessing\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Test Set Receiver Operating Characteristic (ROC) Curve (Optimized SVM)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('svm_test_roc_curve.png') # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3548c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "This section analyzes which features were most influential for the best SVM model's predictions. The approach depends on the kernel type determined by the grid search:\n",
    "\n",
    "* **Check Kernel Type:** Identify the kernel (`linear` or `rbf`) used by the `best_svm_pipeline`.\n",
    "* **Linear Kernel:**\n",
    "    * If the kernel is linear, extract the model coefficients (`.coef_`). Coefficients represent the weight assigned to each feature; larger absolute values indicate higher importance for the linear separation.\n",
    "    * Create a DataFrame showing each feature and its corresponding coefficient.\n",
    "    * Rank features by the absolute value of their coefficients.\n",
    "    * Plot the top 10 most important features based on absolute coefficient values.\n",
    "* **Non-Linear Kernel (e.g., RBF):**\n",
    "    * If the kernel is non-linear, coefficients are not directly interpretable in the original feature space.\n",
    "    * Calculate feature importance using `permutation_importance` on the test set. This method measures the decrease in model score (F1 score, in this case) when a single feature's values are randomly shuffled across multiple repeats.\n",
    "    * Organize the mean importance scores and standard deviations into a DataFrame.\n",
    "    * Plot the top 10 most important features based on mean permutation importance, including error bars representing the standard deviation across repeats.\n",
    "* **Save Plot:** Save the generated feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b158a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Importance --- \n",
    "# Determine feature importance based on the best model's kernel type\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot determine feature importance.\")\n",
    "elif 'X_train' not in locals() or 'X_test' not in locals(): # Check if X_train/X_test exist\n",
    "     print(\"\\nError: Training or Test data not available. Cannot determine feature importance.\")\n",
    "else:\n",
    "    # Get the SVM step from the best pipeline\n",
    "    best_svm_model = best_svm_pipeline.named_steps['svm']\n",
    "    best_kernel = best_svm_model.kernel\n",
    "    print(f\"\\nBest model kernel type: {best_kernel}\")\n",
    "\n",
    "    if best_kernel == 'linear':\n",
    "        print(\"Calculating feature importance using Linear SVM coefficients...\")\n",
    "        # For linear SVM, coefficients indicate feature importance\n",
    "        coefficients = best_svm_model.coef_[0]\n",
    "        \n",
    "        # Create DataFrame to display feature names and coefficients\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns, # Get feature names from training data\n",
    "            'Coefficient': coefficients\n",
    "        })\n",
    "        # Use absolute coefficient value to rank importance regardless of direction\n",
    "        feature_importance_df['Abs_Coefficient'] = abs(feature_importance_df['Coefficient'])\n",
    "        # Sort features by importance (descending)\n",
    "        feature_importance_df = feature_importance_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance (Top 10 based on Linear SVM Coefficients):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.barplot(x='Abs_Coefficient', y='Feature', data=feature_importance_df.head(10), palette='viridis')\n",
    "        plt.title('Top 10 Features by Absolute Coefficient (Linear SVM)')\n",
    "        plt.xlabel('Absolute Coefficient Value')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('svm_linear_feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "    else: # For non-linear kernels like 'rbf'\n",
    "        print(\"\\nCalculating feature importance using Permutation Importance (on test set)...\")\n",
    "        # Permutation importance is suitable for non-linear models\n",
    "        # It assesses importance by measuring performance drop when a feature is shuffled\n",
    "        perm_importance = permutation_importance(\n",
    "            best_svm_pipeline, # Use the entire pipeline (includes scaling)\n",
    "            X_test, \n",
    "            y_test, \n",
    "            n_repeats=30,       # Increase repeats for more stable estimates\n",
    "            random_state=42,\n",
    "            scoring='f1',       # Use the primary evaluation metric\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Organize results into a DataFrame\n",
    "        sorted_idx = perm_importance.importances_mean.argsort()[::-1] # Sort descending by mean importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns[sorted_idx],\n",
    "            'Importance': perm_importance.importances_mean[sorted_idx],\n",
    "            'Std Dev': perm_importance.importances_std[sorted_idx]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nFeature Importance (Top 10 based on Permutation Importance):\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        # Plot permutation importance with error bars\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        ax = sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10), \n",
    "                    palette='viridis')\n",
    "        # Add error bars using calculated standard deviation\n",
    "        ax.errorbar(x=feature_importance_df['Importance'].head(10), \n",
    "                    y=np.arange(10), \n",
    "                    xerr=feature_importance_df['Std Dev'].head(10), \n",
    "                    fmt='none', c='black', capsize=3)\n",
    "        plt.title(f'Top 10 Features by Permutation Importance ({best_kernel.upper()} SVM)')\n",
    "        plt.xlabel('Mean Importance (F1 Score Decrease)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'svm_{best_kernel}_feature_importance.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d7886",
   "metadata": {},
   "source": [
    "## Cross-Validation and Baseline Comparison\n",
    "\n",
    "This section provides further evaluation and context for the optimized SVM model. It includes:\n",
    "\n",
    "* **Cross-Validation on Full Dataset:**\n",
    "    * Performs k-fold cross-validation (using the same `StratifiedKFold` strategy as the grid search) on the *entire* dataset (`X`, `y`) using the `best_svm_pipeline`.\n",
    "    * Calculates the F1 score for each fold.\n",
    "    * Reports the mean and standard deviation of the cross-validation F1 scores. This gives an estimate of the model's stability and expected performance on different subsets of the data.\n",
    "* **Baseline Model Comparison:**\n",
    "    * Defines two baseline SVM models within pipelines: one with a linear kernel and default parameters, and one with an RBF kernel and default parameters (both using `StandardScaler`).\n",
    "    * Trains these baseline models on the training set (`X_train`, `y_train`).\n",
    "    * Evaluates the baselines on the test set (`X_test`, `y_test`), calculating Accuracy, F1 Score, Precision, Recall, and Training Time.\n",
    "    * Combines the results of the baseline models and the previously evaluated `Optimized SVM` into a DataFrame.\n",
    "    * Prints the comparison table.\n",
    "    * Generates bar plots comparing the performance metrics (Accuracy, F1, Precision, Recall) and the training times of the optimized model versus the baselines.\n",
    "    * Saves the comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-validation for Reliability Estimation --- \n",
    "# Perform cross-validation on the *entire* dataset using the *best found pipeline*\n",
    "# This gives an estimate of how the model might perform on average on new, unseen data splits\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot perform cross-validation.\")\n",
    "elif X.empty or y.empty:\n",
    "     print(\"\\nError: Full dataset (X, y) not available or empty. Cannot perform cross-validation.\")\n",
    "else:\n",
    "    print(\"\\nPerforming cross-validation on the full dataset with the best pipeline...\")\n",
    "    # Use the same CV strategy as in grid search for consistency\n",
    "    cv_scores = cross_val_score(best_svm_pipeline, X, y, cv=cv, scoring='f1', n_jobs=-1)\n",
    "    print(f\"\\nCross-validation F1 scores for each fold: {cv_scores}\")\n",
    "    print(f\"Mean CV F1 score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# --- Compare with Baseline Models --- \n",
    "print(\"\\nComparing with baseline models (untuned, default parameters)...\")\n",
    "\n",
    "# Define Baseline Linear SVM (using default C=1)\n",
    "baseline_linear = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Use standard scaler for baseline consistency\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Define Baseline RBF SVM (using default C=1, gamma='scale')\n",
    "baseline_rbf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Dictionary to store results for comparison\n",
    "baseline_results = {}\n",
    "baseline_models = {\n",
    "    'Baseline Linear SVM': baseline_linear,\n",
    "    'Baseline RBF SVM': baseline_rbf\n",
    "}\n",
    "\n",
    "# Check if training and test data are valid before proceeding\n",
    "if 'X_train' not in locals() or 'y_train' not in locals() or X_train.empty or y_train.empty:\n",
    "    print(\"\\nError: Training data not available. Cannot train baseline models.\")\n",
    "elif 'X_test' not in locals() or 'y_test' not in locals() or X_test.empty or y_test.empty:\n",
    "    print(\"\\nError: Test data not available. Cannot evaluate baseline models.\")\n",
    "else:\n",
    "    # Train and evaluate baseline models\n",
    "    for name, model in baseline_models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Evaluating {name} on test set...\")\n",
    "        y_pred_base = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred_base)\n",
    "        # Get classification report as dict, handle potential zero division\n",
    "        report = classification_report(y_test, y_pred_base, output_dict=True, zero_division=0)\n",
    "        \n",
    "        # Store key metrics\n",
    "        baseline_results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': report['weighted avg']['f1-score'],\n",
    "            'Precision': report['weighted avg']['precision'],\n",
    "            'Recall': report['weighted avg']['recall'],\n",
    "            'Training Time (s)': train_time\n",
    "        }\n",
    "\n",
    "    # Add the optimized model's results (calculated previously) for direct comparison\n",
    "    if 'best_svm_pipeline' in locals() and 'test_accuracy' in locals():\n",
    "        baseline_results['Optimized SVM'] = {\n",
    "            'Accuracy': test_accuracy, \n",
    "            'F1 Score': f1, \n",
    "            'Precision': precision, \n",
    "            'Recall': sensitivity, \n",
    "            'Training Time (s)': grid_search_time # Use the stored time for the best grid search run\n",
    "        }\n",
    "    \n",
    "    # Convert results dictionary to DataFrame for display\n",
    "    results_comparison_df = pd.DataFrame(baseline_results).T\n",
    "    print(\"\\n--- Model Comparison Table ---\")\n",
    "    print(results_comparison_df)\n",
    "    \n",
    "    # --- Plotting Comparisons --- \n",
    "    plt.style.use('seaborn-v0_8-talk') # Use a presentation-friendly style\n",
    "\n",
    "    # Plot performance metrics comparison\n",
    "    fig_perf, ax_perf = plt.subplots(figsize=(12, 7))\n",
    "    results_comparison_df[['Accuracy', 'F1 Score', 'Precision', 'Recall']].plot(kind='bar', ax=ax_perf)\n",
    "    ax_perf.set_title('Model Performance Comparison (Test Set)')\n",
    "    ax_perf.set_ylabel('Score')\n",
    "    ax_perf.set_xlabel('Model')\n",
    "    ax_perf.tick_params(axis='x', rotation=15)\n",
    "    ax_perf.legend(title='Metric', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "    ax_perf.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_model_comparison_performance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training time comparison\n",
    "    fig_time, ax_time = plt.subplots(figsize=(10, 5))\n",
    "    results_comparison_df[['Training Time (s)']].plot(kind='bar', ax=ax_time, legend=False, color='skyblue')\n",
    "    ax_time.set_title('Model Training Time Comparison')\n",
    "    ax_time.set_ylabel('Time (seconds)')\n",
    "    ax_time.set_xlabel('Model')\n",
    "    ax_time.tick_params(axis='x', rotation=15)\n",
    "    ax_time.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    # Add value labels to bars for clarity\n",
    "    for container in ax_time.containers:\n",
    "        ax_time.bar_label(container, fmt='%.2f')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_model_comparison_time.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placeholder_model2_markdown",
   "metadata": {},
   "source": [
    "## Second Machine Learning Technique: [Your Second Model Name - e.g., Neural Network (MLP)]\n",
    "\n",
    "*(Project Requirement: Implement and evaluate at least two ML techniques)*\n",
    "\n",
    "This section implements and evaluates the second chosen machine learning technique for comparison with the SVM.\n",
    "\n",
    "* **Setup:** Define the pipeline (including scaling) and parameter grid for the second model (e.g., MLPClassifier).\n",
    "* **Hyperparameter Tuning:** Use `GridSearchCV` with the same cross-validation strategy (`cv`) and scoring metric (`f1`) to find the best hyperparameters for this model.\n",
    "* **Evaluation:** Evaluate the best version of this second model on the held-out test set (`X_test`, `y_test`), calculating the same set of metrics (Accuracy, CM, Precision, Recall, F1, ROC AUC) as used for the SVM.\n",
    "* **Feature Importance (If Applicable):** Calculate and visualize feature importance using an appropriate method for the chosen model (e.g., permutation importance).\n",
    "* **Learning Curve:** Generate a learning curve for the optimized second model.\n",
    "* **Add to Comparison:** Add the results (performance metrics, training time) of this optimized second model to the `results_comparison_df` DataFrame and regenerate the comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder_model2_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for implementing and evaluating the second ML technique\n",
    "# Example using MLPClassifier (as potentially proposed)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "print(\"\\n--- Implementing Second Technique: MLP Classifier ---\")\n",
    "\n",
    "# 1. Define Pipeline for MLP\n",
    "pipeline_mlp = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Or RobustScaler, can be tuned\n",
    "    ('mlp', MLPClassifier(random_state=42, max_iter=1000)) # Increased max_iter\n",
    "])\n",
    "\n",
    "# 2. Define Parameter Grid for MLP (Example)\n",
    "param_grid_mlp = {\n",
    "    'scaler': [StandardScaler(), RobustScaler()],\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50)], # Example architectures\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam'], # Adam is often a good default\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01] # Regularization\n",
    "}\n",
    "\n",
    "# 3. Perform GridSearchCV for MLP\n",
    "print(\"\\nPerforming grid search for MLP Classifier...\")\n",
    "grid_search_mlp = GridSearchCV(\n",
    "    pipeline_mlp,\n",
    "    param_grid_mlp,\n",
    "    cv=cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time_mlp = time.time()\n",
    "# Ensure training data is available\n",
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    grid_search_mlp.fit(X_train, y_train)\n",
    "    grid_search_time_mlp = time.time() - start_time_mlp\n",
    "    print(f\"Grid search (MLP) completed in {grid_search_time_mlp:.2f} seconds.\")\n",
    "    print(f\"Best MLP parameters: {grid_search_mlp.best_params_}\")\n",
    "    print(f\"Best cross-validation F1 score (MLP): {grid_search_mlp.best_score_:.4f}\")\n",
    "    best_mlp_pipeline = grid_search_mlp.best_estimator_\n",
    "\n",
    "    # 4. Evaluate Best MLP on Test Set\n",
    "    print(\"\\nEvaluating best MLP model on the test set...\")\n",
    "    y_pred_mlp_test = best_mlp_pipeline.predict(X_test)\n",
    "    # y_prob_mlp_test = best_mlp_pipeline.predict_proba(X_test)[:, 1] # If needed for ROC\n",
    "    \n",
    "    mlp_accuracy = accuracy_score(y_test, y_pred_mlp_test)\n",
    "    mlp_report = classification_report(y_test, y_pred_mlp_test, output_dict=True, zero_division=0)\n",
    "    mlp_f1 = mlp_report['weighted avg']['f1-score']\n",
    "    mlp_precision = mlp_report['weighted avg']['precision']\n",
    "    mlp_recall = mlp_report['weighted avg']['recall']\n",
    "\n",
    "    print(f\"MLP Test Accuracy: {mlp_accuracy:.4f}\")\n",
    "    print(f\"MLP Test F1 Score: {mlp_f1:.4f}\")\n",
    "    print(\"MLP Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_mlp_test, zero_division=0))\n",
    "    \n",
    "    # 5. Add MLP results to the comparison DataFrame (if it exists)\n",
    "    if 'results_comparison_df' in locals():\n",
    "        results_comparison_df.loc['Optimized MLP'] = {\n",
    "            'Accuracy': mlp_accuracy,\n",
    "            'F1 Score': mlp_f1,\n",
    "            'Precision': mlp_precision,\n",
    "            'Recall': mlp_recall,\n",
    "            'Training Time (s)': grid_search_time_mlp\n",
    "        }\n",
    "        print(\"\\n--- Updated Model Comparison Table ---\")\n",
    "        print(results_comparison_df)\n",
    "        \n",
    "        # Regenerate comparison plots if needed\n",
    "        # ... (plotting code similar to the end of the SVM baseline comparison cell) ...\n",
    "    else:\n",
    "        print(\"Comparison DataFrame not found, skipping update.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping MLP implementation due to missing training data.\")\n",
    "\n",
    "# Feature Importance for MLP using Permutation Importance\n",
    "if 'best_mlp_pipeline' in locals() and 'X_test' in locals() and not X_test.empty:\n",
    "    print(\"\\nCalculating feature importance for MLP using Permutation Importance...\")\n",
    "    \n",
    "    # Calculate permutation importance on test set\n",
    "    perm_importance_mlp = permutation_importance(\n",
    "        best_mlp_pipeline,\n",
    "        X_test, \n",
    "        y_test, \n",
    "        n_repeats=30,\n",
    "        random_state=42,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Organize results into DataFrame\n",
    "    sorted_idx_mlp = perm_importance_mlp.importances_mean.argsort()[::-1]\n",
    "    feature_importance_mlp_df = pd.DataFrame({\n",
    "        'Feature': X_test.columns[sorted_idx_mlp],\n",
    "        'Importance': perm_importance_mlp.importances_mean[sorted_idx_mlp],\n",
    "        'Std Dev': perm_importance_mlp.importances_std[sorted_idx_mlp]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nMLP Feature Importance (Top 10):\")\n",
    "    print(feature_importance_mlp_df.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = sns.barplot(x='Importance', y='Feature', data=feature_importance_mlp_df.head(10), \n",
    "                palette='viridis')\n",
    "    # Add error bars\n",
    "    ax.errorbar(x=feature_importance_mlp_df['Importance'].head(10), \n",
    "                y=np.arange(10), \n",
    "                xerr=feature_importance_mlp_df['Std Dev'].head(10), \n",
    "                fmt='none', c='black', capsize=3)\n",
    "    plt.title('Top 10 Features by Permutation Importance (MLP)')\n",
    "    plt.xlabel('Mean Importance (F1 Score Decrease)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mlp_feature_importance.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Learning Curve for MLP\n",
    "    print(\"\\nGenerating learning curve for MLP...\")\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        best_mlp_pipeline,\n",
    "        X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Calculate statistics\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Cross-validation score')\n",
    "    \n",
    "    # Add standard deviation bands\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.title('Learning Curve (MLP Classifier)')\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mlp_learning_curve.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot generate MLP feature importance or learning curve - model or test data not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7377f",
   "metadata": {},
   "source": [
    "## Learning Curve Analysis\n",
    "\n",
    "This cell generates a learning curve for the best SVM pipeline identified by the grid search. Learning curves help diagnose model performance issues related to bias and variance, and indicate whether collecting more data might be beneficial. The steps are:\n",
    "\n",
    "* **Data Check:** Ensure the best pipeline and the full dataset (`X`, `y`) are available.\n",
    "* **Generate Learning Curve Data:** Use the `learning_curve` function from scikit-learn.\n",
    "    * Pass the `best_svm_pipeline`, the full dataset (`X`, `y`), and the cross-validation strategy (`cv`).\n",
    "    * Specify `train_sizes` (e.g., 10 points from 10% to 100% of the training data) to evaluate performance on different dataset sizes.\n",
    "    * Use F1 score for evaluation, consistent with the grid search optimization metric.\n",
    "    * This function returns the training sizes used, and the corresponding training scores and cross-validation scores for each size across the different CV folds.\n",
    "* **Calculate Statistics:** Compute the mean and standard deviation of the training and cross-validation scores across the folds for each training size.\n",
    "* **Plot Learning Curve:**\n",
    "    * Plot the mean training score and mean cross-validation score against the number of training examples.\n",
    "    * Include shaded areas representing ± one standard deviation around the mean scores (using `fill_between`) to show the variability of the scores.\n",
    "    * Add title, labels, legend, and grid for clarity.\n",
    "* **Save Plot:** Save the generated learning curve plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ee18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Learning Curve --- \n",
    "# Generate learning curves for the best SVM model to understand bias/variance trade-off\n",
    "\n",
    "if 'best_svm_pipeline' not in locals():\n",
    "     print(\"\\nError: Best SVM pipeline not found. Cannot generate learning curve.\")\n",
    "elif X.empty or y.empty:\n",
    "     print(\"\\nError: Full dataset (X, y) not available or empty. Cannot generate learning curve.\")\n",
    "else:\n",
    "    print(\"\\nGenerating learning curves for the best SVM pipeline...\")\n",
    "    # Generate data points for the learning curve\n",
    "    train_sizes_abs, train_scores, test_scores = learning_curve(\n",
    "        best_svm_pipeline, # The best estimator pipeline found\n",
    "        X,                # Full feature set\n",
    "        y,                # Full target variable\n",
    "        cv=cv,            # Use the same cross-validation strategy\n",
    "        n_jobs=-1,        # Use all available cores\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), # Evaluate on 10 different training set sizes\n",
    "        scoring='f1',     # Evaluate performance using F1 score\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Calculate mean and standard deviation of scores across folds\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    # Plot the learning curve\n",
    "    plt.style.use('seaborn-v0_8-whitegrid') # Apply a clean plot style\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot shaded regions for standard deviation\n",
    "    plt.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes_abs, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    \n",
    "    # Plot mean scores\n",
    "    plt.plot(train_sizes_abs, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes_abs, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    # Add plot labels and title\n",
    "    plt.title(\"Learning Curve (SVM)\")\n",
    "    plt.xlabel(\"Number of Training Examples\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('svm_learning_curve.png') # Save the plot\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSVM analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "*TODO*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
